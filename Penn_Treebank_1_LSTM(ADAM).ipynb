{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Penn_Treebank_1_LSTM(ADAM).ipynb","provenance":[{"file_id":"1C7tl3uUJ2eh-A39b93b0JPx8qy8RuYdN","timestamp":1590433263405},{"file_id":"18_0qHuK5tvwAfyVVoruWVta1pxBzDQxC","timestamp":1590419745097}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"S6lyfG8jZt1b","colab_type":"text"},"source":["# Init"]},{"cell_type":"code","metadata":{"id":"usrX2f-qZvi5","colab_type":"code","outputId":"b44eae81-db0a-4ee2-ed30-6f1a57781b43","executionInfo":{"status":"ok","timestamp":1590433933193,"user_tz":-60,"elapsed":9164,"user":{"displayName":"Eric Lee","photoUrl":"","userId":"15439544951905613244"}},"colab":{"base_uri":"https://localhost:8080/","height":168}},"source":["# Execute this code block to install dependencies when running on colab\n","try:\n","    import torch\n","except:\n","    from os.path import exists\n","    from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n","    platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n","    cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n","    accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n","\n","    !pip install -q http://download.pytorch.org/whl/{accelerator}/torch-1.0.0-{platform}-linux_x86_64.whl torchvision\n","\n","try: \n","    import torchbearer\n","except:\n","    !pip install torchbearer"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting torchbearer\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/e9/4049a47dd2e5b6346a2c5d215b0c67dce814afbab1cd54ce024533c4834e/torchbearer-0.5.3-py3-none-any.whl (138kB)\n","\r\u001b[K     |██▍                             | 10kB 17.3MB/s eta 0:00:01\r\u001b[K     |████▊                           | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 40kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 71kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 81kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 92kB 3.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 143kB 2.8MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchbearer) (1.18.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchbearer) (4.41.1)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from torchbearer) (1.5.0+cu101)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->torchbearer) (0.16.0)\n","Installing collected packages: torchbearer\n","Successfully installed torchbearer-0.5.3\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RQV9LbVMZ5NG","colab_type":"text"},"source":["# Prepare Penn Treebank dataset"]},{"cell_type":"code","metadata":{"id":"KhEeh6x8Z9eE","colab_type":"code","colab":{}},"source":["# automatically reload external modules if they change\n","%load_ext autoreload\n","%autoreload 2\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torchtext import data\n","from torchtext import vocab\n","from torchtext import datasets\n","\n","import numpy as np\n","from matplotlib import pyplot as plt\n","\n","from tqdm import tqdm"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NWmejPqvd93t","colab_type":"code","outputId":"a5281436-9197-4484-dcd1-e67933b596b8","executionInfo":{"status":"ok","timestamp":1590434411284,"user_tz":-60,"elapsed":487238,"user":{"displayName":"Eric Lee","photoUrl":"","userId":"15439544951905613244"}},"colab":{"base_uri":"https://localhost:8080/","height":168}},"source":["tokenize = lambda x: x.split()\n","TEXT = data.Field(sequential = True, tokenize = tokenize, lower = True, batch_first = True)\n","train_dataset, val_dataset, test_dataset = datasets.PennTreebank.splits(TEXT)\n","TEXT.build_vocab(train_dataset, vectors=vocab.GloVe(name='6B', dim=300))\n","\n","vocab_size = len(TEXT.vocab)\n","word_embeddings = TEXT.vocab.vectors\n","print(vocab_size)\n","print(word_embeddings.size())\n","embeddings_length = 300\n","hidden_size = 256\n","batch_size = 32"],"execution_count":0,"outputs":[{"output_type":"stream","text":["downloading ptb.train.txt\n"],"name":"stdout"},{"output_type":"stream","text":["ptb.train.txt: 5.10MB [00:00, 26.7MB/s]\n"],"name":"stderr"},{"output_type":"stream","text":["downloading ptb.valid.txt\n"],"name":"stdout"},{"output_type":"stream","text":["ptb.valid.txt: 400kB [00:00, 10.2MB/s]                   \n"],"name":"stderr"},{"output_type":"stream","text":["downloading ptb.test.txt\n"],"name":"stdout"},{"output_type":"stream","text":["ptb.test.txt: 450kB [00:00, 11.0MB/s]                   \n",".vector_cache/glove.6B.zip: 862MB [06:30, 2.21MB/s]                           \n","100%|█████████▉| 399441/400000 [00:51<00:00, 7814.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["10001\n","torch.Size([10001, 300])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5Hl-iXexfSWl","colab_type":"code","colab":{}},"source":["train_iter, val_iter, test_iter = data.BPTTIterator.splits((train_dataset, val_dataset, test_dataset), batch_size = 32, bptt_len=30, repeat=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wPLZIYqKhvTF","colab_type":"code","colab":{}},"source":["class LstmLangModel(nn.Module):\n","   def __init__(self, batch_size, hidden_size, vocab_size, embeddings_length, weights):\n","       super(LstmLangModel, self).__init__()\n","       self.batch_size = batch_size\n","       self.hidden_size = hidden_size\n","       self.vocab_size = vocab_size\n","       self.embed = nn.Embedding(vocab_size, embeddings_length)\n","       self.embed.weight.data.copy_(weights)\n","       self.lstm = nn.LSTM(embeddings_length, hidden_size, batch_first=True)\n","       self.fc = nn.Linear(hidden_size, vocab_size)\n","   def forward(self, x, h):\n","       x = self.embed(x)\n","       output_seq, (h, c) = self.lstm(x, h)\n","       out = output_seq.reshape(output_seq.size(0)*output_seq.size(1), output_seq.size(2))\n","       out = self.fc(out)\n","       return out, (h, c)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ySgb_BtfkbjZ","colab_type":"code","outputId":"15a1516f-22ad-4c17-f8db-bf475ad64d5f","executionInfo":{"status":"ok","timestamp":1590434411286,"user_tz":-60,"elapsed":487226,"user":{"displayName":"Eric Lee","photoUrl":"","userId":"15439544951905613244"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["vocab_size = len(TEXT.vocab)\n","word_embeddings = TEXT.vocab.vectors\n","print(vocab_size)\n","print(word_embeddings.size())\n","embeddings_length = 300\n","hidden_size = 256\n","batch_size = 32"],"execution_count":0,"outputs":[{"output_type":"stream","text":["10001\n","torch.Size([10001, 300])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"O7h9zfUYkFEc","colab_type":"code","outputId":"b0a20c38-a11f-406c-a319-148ca4ffd935","executionInfo":{"status":"ok","timestamp":1590434411287,"user_tz":-60,"elapsed":487218,"user":{"displayName":"Eric Lee","photoUrl":"","userId":"15439544951905613244"}},"colab":{"base_uri":"https://localhost:8080/","height":101}},"source":["model = LstmLangModel(batch_size, hidden_size, vocab_size, embeddings_length, word_embeddings)\n","model.eval()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LstmLangModel(\n","  (embed): Embedding(10001, 300)\n","  (lstm): LSTM(300, 256, batch_first=True)\n","  (fc): Linear(in_features=256, out_features=10001, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"k6CWQqenmQ56","colab_type":"text"},"source":["# Torchbear Section"]},{"cell_type":"code","metadata":{"id":"TtOu1hMbjzbZ","colab_type":"code","outputId":"5b3177cf-cf3b-4b96-e8c8-8933f5227682","executionInfo":{"status":"ok","timestamp":1590438991546,"user_tz":-60,"elapsed":5067467,"user":{"displayName":"Eric Lee","photoUrl":"","userId":"15439544951905613244"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","net_lstm = LstmLangModel(batch_size, hidden_size, vocab_size, embeddings_length, word_embeddings)\n","net_lstm = net_lstm.to(device)\n","criterion = nn.CrossEntropyLoss()\n","optim = torch.optim.Adam(filter(lambda p: p.requires_grad, net_lstm.parameters()),lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n","num_epochs = 200\n","epoch_list = []\n","train_loss_lstm_list = []\n","train_perp_lstm_list = []\n","\n","def detach(states):\n","   return [state.detach() for state in states]\n","\n","\n","\n","for epoch in range(num_epochs):\n","   train_loss = 0\n","   states = (torch.zeros(1, batch_size, hidden_size).to(device),\n","             torch.zeros(1, batch_size, hidden_size).to(device))\n","   net_lstm.train()\n","\n","   for i, batch in enumerate(train_iter):\n","       text = batch.text.to(device)\n","       labels = batch.target.to(device)\n","       text = text.permute(1, 0)\n","       labels = labels.permute(1, 0)\n","\n","       optim.zero_grad()\n","       states = detach(states)\n","       outputs, states = net_lstm(text, states)\n","       loss = criterion(outputs, labels.reshape(-1))\n","       train_loss += loss.item()\n","       loss.backward()\n","       optim.step()\n","   avg_train_loss = train_loss / len(train_iter)\n","   perplexity = np.exp(avg_train_loss)\n","   print('Epoch [{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'.format(epoch + 1, num_epochs, avg_train_loss, perplexity))\n","   train_loss_lstm_list.append(avg_train_loss)\n","   train_perp_lstm_list.append(perplexity)\n","\n","   if epoch % 100 == 0:\n","       torch.save(net_lstm.state_dict(), r\"./LSTM_\" + str(epoch) + r\".pth\")\n","   \n","   if epoch == num_epochs - 1:\n","       torch.save(net_lstm.state_dict(), r\"./LSTM_\" + str(epoch) + r\".pth\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\r100%|█████████▉| 399441/400000 [01:10<00:00, 7814.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch [1/200], Loss: 5.7908, Perplexity: 327.26\n","Epoch [2/200], Loss: 5.0649, Perplexity: 158.36\n","Epoch [3/200], Loss: 4.7537, Perplexity: 116.01\n","Epoch [4/200], Loss: 4.5301, Perplexity: 92.77\n","Epoch [5/200], Loss: 4.3512, Perplexity: 77.57\n","Epoch [6/200], Loss: 4.2001, Perplexity: 66.69\n","Epoch [7/200], Loss: 4.0673, Perplexity: 58.40\n","Epoch [8/200], Loss: 3.9478, Perplexity: 51.82\n","Epoch [9/200], Loss: 3.8388, Perplexity: 46.47\n","Epoch [10/200], Loss: 3.7384, Perplexity: 42.03\n","Epoch [11/200], Loss: 3.6451, Perplexity: 38.29\n","Epoch [12/200], Loss: 3.5575, Perplexity: 35.07\n","Epoch [13/200], Loss: 3.4744, Perplexity: 32.28\n","Epoch [14/200], Loss: 3.3948, Perplexity: 29.81\n","Epoch [15/200], Loss: 3.3187, Perplexity: 27.62\n","Epoch [16/200], Loss: 3.2467, Perplexity: 25.71\n","Epoch [17/200], Loss: 3.1793, Perplexity: 24.03\n","Epoch [18/200], Loss: 3.1165, Perplexity: 22.57\n","Epoch [19/200], Loss: 3.0566, Perplexity: 21.26\n","Epoch [20/200], Loss: 2.9995, Perplexity: 20.07\n","Epoch [21/200], Loss: 2.9457, Perplexity: 19.02\n","Epoch [22/200], Loss: 2.8936, Perplexity: 18.06\n","Epoch [23/200], Loss: 2.8441, Perplexity: 17.19\n","Epoch [24/200], Loss: 2.7975, Perplexity: 16.40\n","Epoch [25/200], Loss: 2.7522, Perplexity: 15.68\n","Epoch [26/200], Loss: 2.7067, Perplexity: 14.98\n","Epoch [27/200], Loss: 2.6635, Perplexity: 14.35\n","Epoch [28/200], Loss: 2.6231, Perplexity: 13.78\n","Epoch [29/200], Loss: 2.5843, Perplexity: 13.25\n","Epoch [30/200], Loss: 2.5461, Perplexity: 12.76\n","Epoch [31/200], Loss: 2.5120, Perplexity: 12.33\n","Epoch [32/200], Loss: 2.4786, Perplexity: 11.92\n","Epoch [33/200], Loss: 2.4454, Perplexity: 11.54\n","Epoch [34/200], Loss: 2.4141, Perplexity: 11.18\n","Epoch [35/200], Loss: 2.3839, Perplexity: 10.85\n","Epoch [36/200], Loss: 2.3561, Perplexity: 10.55\n","Epoch [37/200], Loss: 2.3281, Perplexity: 10.26\n","Epoch [38/200], Loss: 2.3017, Perplexity:  9.99\n","Epoch [39/200], Loss: 2.2755, Perplexity:  9.73\n","Epoch [40/200], Loss: 2.2503, Perplexity:  9.49\n","Epoch [41/200], Loss: 2.2264, Perplexity:  9.27\n","Epoch [42/200], Loss: 2.2041, Perplexity:  9.06\n","Epoch [43/200], Loss: 2.1848, Perplexity:  8.89\n","Epoch [44/200], Loss: 2.1621, Perplexity:  8.69\n","Epoch [45/200], Loss: 2.1410, Perplexity:  8.51\n","Epoch [46/200], Loss: 2.1214, Perplexity:  8.34\n","Epoch [47/200], Loss: 2.1024, Perplexity:  8.19\n","Epoch [48/200], Loss: 2.0849, Perplexity:  8.04\n","Epoch [49/200], Loss: 2.0656, Perplexity:  7.89\n","Epoch [50/200], Loss: 2.0485, Perplexity:  7.76\n","Epoch [51/200], Loss: 2.0329, Perplexity:  7.64\n","Epoch [52/200], Loss: 2.0170, Perplexity:  7.52\n","Epoch [53/200], Loss: 2.0001, Perplexity:  7.39\n","Epoch [54/200], Loss: 1.9856, Perplexity:  7.28\n","Epoch [55/200], Loss: 1.9703, Perplexity:  7.17\n","Epoch [56/200], Loss: 1.9564, Perplexity:  7.07\n","Epoch [57/200], Loss: 1.9448, Perplexity:  6.99\n","Epoch [58/200], Loss: 1.9313, Perplexity:  6.90\n","Epoch [59/200], Loss: 1.9178, Perplexity:  6.81\n","Epoch [60/200], Loss: 1.9045, Perplexity:  6.72\n","Epoch [61/200], Loss: 1.8938, Perplexity:  6.64\n","Epoch [62/200], Loss: 1.8826, Perplexity:  6.57\n","Epoch [63/200], Loss: 1.8702, Perplexity:  6.49\n","Epoch [64/200], Loss: 1.8585, Perplexity:  6.41\n","Epoch [65/200], Loss: 1.8483, Perplexity:  6.35\n","Epoch [66/200], Loss: 1.8380, Perplexity:  6.28\n","Epoch [67/200], Loss: 1.8298, Perplexity:  6.23\n","Epoch [68/200], Loss: 1.8201, Perplexity:  6.17\n","Epoch [69/200], Loss: 1.8110, Perplexity:  6.12\n","Epoch [70/200], Loss: 1.7998, Perplexity:  6.05\n","Epoch [71/200], Loss: 1.7918, Perplexity:  6.00\n","Epoch [72/200], Loss: 1.7826, Perplexity:  5.95\n","Epoch [73/200], Loss: 1.7749, Perplexity:  5.90\n","Epoch [74/200], Loss: 1.7661, Perplexity:  5.85\n","Epoch [75/200], Loss: 1.7582, Perplexity:  5.80\n","Epoch [76/200], Loss: 1.7510, Perplexity:  5.76\n","Epoch [77/200], Loss: 1.7421, Perplexity:  5.71\n","Epoch [78/200], Loss: 1.7339, Perplexity:  5.66\n","Epoch [79/200], Loss: 1.7272, Perplexity:  5.62\n","Epoch [80/200], Loss: 1.7206, Perplexity:  5.59\n","Epoch [81/200], Loss: 1.7139, Perplexity:  5.55\n","Epoch [82/200], Loss: 1.7073, Perplexity:  5.51\n","Epoch [83/200], Loss: 1.7008, Perplexity:  5.48\n","Epoch [84/200], Loss: 1.6934, Perplexity:  5.44\n","Epoch [85/200], Loss: 1.6868, Perplexity:  5.40\n","Epoch [86/200], Loss: 1.6816, Perplexity:  5.37\n","Epoch [87/200], Loss: 1.6769, Perplexity:  5.35\n","Epoch [88/200], Loss: 1.6688, Perplexity:  5.31\n","Epoch [89/200], Loss: 1.6628, Perplexity:  5.27\n","Epoch [90/200], Loss: 1.6586, Perplexity:  5.25\n","Epoch [91/200], Loss: 1.6520, Perplexity:  5.22\n","Epoch [92/200], Loss: 1.6458, Perplexity:  5.19\n","Epoch [93/200], Loss: 1.6399, Perplexity:  5.15\n","Epoch [94/200], Loss: 1.6366, Perplexity:  5.14\n","Epoch [95/200], Loss: 1.6296, Perplexity:  5.10\n","Epoch [96/200], Loss: 1.6245, Perplexity:  5.08\n","Epoch [97/200], Loss: 1.6189, Perplexity:  5.05\n","Epoch [98/200], Loss: 1.6170, Perplexity:  5.04\n","Epoch [99/200], Loss: 1.6125, Perplexity:  5.02\n","Epoch [100/200], Loss: 1.6071, Perplexity:  4.99\n","Epoch [101/200], Loss: 1.6009, Perplexity:  4.96\n","Epoch [102/200], Loss: 1.5994, Perplexity:  4.95\n","Epoch [103/200], Loss: 1.5947, Perplexity:  4.93\n","Epoch [104/200], Loss: 1.5878, Perplexity:  4.89\n","Epoch [105/200], Loss: 1.5818, Perplexity:  4.86\n","Epoch [106/200], Loss: 1.5781, Perplexity:  4.85\n","Epoch [107/200], Loss: 1.5745, Perplexity:  4.83\n","Epoch [108/200], Loss: 1.5694, Perplexity:  4.80\n","Epoch [109/200], Loss: 1.5699, Perplexity:  4.81\n","Epoch [110/200], Loss: 1.5628, Perplexity:  4.77\n","Epoch [111/200], Loss: 1.5563, Perplexity:  4.74\n","Epoch [112/200], Loss: 1.5542, Perplexity:  4.73\n","Epoch [113/200], Loss: 1.5524, Perplexity:  4.72\n","Epoch [114/200], Loss: 1.5510, Perplexity:  4.72\n","Epoch [115/200], Loss: 1.5442, Perplexity:  4.68\n","Epoch [116/200], Loss: 1.5388, Perplexity:  4.66\n","Epoch [117/200], Loss: 1.5372, Perplexity:  4.65\n","Epoch [118/200], Loss: 1.5346, Perplexity:  4.64\n","Epoch [119/200], Loss: 1.5314, Perplexity:  4.62\n","Epoch [120/200], Loss: 1.5243, Perplexity:  4.59\n","Epoch [121/200], Loss: 1.5230, Perplexity:  4.59\n","Epoch [122/200], Loss: 1.5195, Perplexity:  4.57\n","Epoch [123/200], Loss: 1.5157, Perplexity:  4.55\n","Epoch [124/200], Loss: 1.5132, Perplexity:  4.54\n","Epoch [125/200], Loss: 1.5109, Perplexity:  4.53\n","Epoch [126/200], Loss: 1.5081, Perplexity:  4.52\n","Epoch [127/200], Loss: 1.5026, Perplexity:  4.49\n","Epoch [128/200], Loss: 1.5007, Perplexity:  4.48\n","Epoch [129/200], Loss: 1.4965, Perplexity:  4.47\n","Epoch [130/200], Loss: 1.4967, Perplexity:  4.47\n","Epoch [131/200], Loss: 1.4935, Perplexity:  4.45\n","Epoch [132/200], Loss: 1.4882, Perplexity:  4.43\n","Epoch [133/200], Loss: 1.4850, Perplexity:  4.42\n","Epoch [134/200], Loss: 1.4830, Perplexity:  4.41\n","Epoch [135/200], Loss: 1.4803, Perplexity:  4.39\n","Epoch [136/200], Loss: 1.4772, Perplexity:  4.38\n","Epoch [137/200], Loss: 1.4744, Perplexity:  4.37\n","Epoch [138/200], Loss: 1.4752, Perplexity:  4.37\n","Epoch [139/200], Loss: 1.4704, Perplexity:  4.35\n","Epoch [140/200], Loss: 1.4693, Perplexity:  4.35\n","Epoch [141/200], Loss: 1.4655, Perplexity:  4.33\n","Epoch [142/200], Loss: 1.4584, Perplexity:  4.30\n","Epoch [143/200], Loss: 1.4584, Perplexity:  4.30\n","Epoch [144/200], Loss: 1.4578, Perplexity:  4.30\n","Epoch [145/200], Loss: 1.4590, Perplexity:  4.30\n","Epoch [146/200], Loss: 1.4490, Perplexity:  4.26\n","Epoch [147/200], Loss: 1.4456, Perplexity:  4.24\n","Epoch [148/200], Loss: 1.4458, Perplexity:  4.25\n","Epoch [149/200], Loss: 1.4460, Perplexity:  4.25\n","Epoch [150/200], Loss: 1.4461, Perplexity:  4.25\n","Epoch [151/200], Loss: 1.4399, Perplexity:  4.22\n","Epoch [152/200], Loss: 1.4384, Perplexity:  4.21\n","Epoch [153/200], Loss: 1.4385, Perplexity:  4.21\n","Epoch [154/200], Loss: 1.4366, Perplexity:  4.21\n","Epoch [155/200], Loss: 1.4327, Perplexity:  4.19\n","Epoch [156/200], Loss: 1.4299, Perplexity:  4.18\n","Epoch [157/200], Loss: 1.4287, Perplexity:  4.17\n","Epoch [158/200], Loss: 1.4261, Perplexity:  4.16\n","Epoch [159/200], Loss: 1.4222, Perplexity:  4.15\n","Epoch [160/200], Loss: 1.4245, Perplexity:  4.16\n","Epoch [161/200], Loss: 1.4207, Perplexity:  4.14\n","Epoch [162/200], Loss: 1.4190, Perplexity:  4.13\n","Epoch [163/200], Loss: 1.4195, Perplexity:  4.14\n","Epoch [164/200], Loss: 1.4109, Perplexity:  4.10\n","Epoch [165/200], Loss: 1.4095, Perplexity:  4.09\n","Epoch [166/200], Loss: 1.4102, Perplexity:  4.10\n","Epoch [167/200], Loss: 1.4087, Perplexity:  4.09\n","Epoch [168/200], Loss: 1.4036, Perplexity:  4.07\n","Epoch [169/200], Loss: 1.4010, Perplexity:  4.06\n","Epoch [170/200], Loss: 1.4030, Perplexity:  4.07\n","Epoch [171/200], Loss: 1.3986, Perplexity:  4.05\n","Epoch [172/200], Loss: 1.3992, Perplexity:  4.05\n","Epoch [173/200], Loss: 1.4023, Perplexity:  4.06\n","Epoch [174/200], Loss: 1.3979, Perplexity:  4.05\n","Epoch [175/200], Loss: 1.3905, Perplexity:  4.02\n","Epoch [176/200], Loss: 1.3927, Perplexity:  4.03\n","Epoch [177/200], Loss: 1.3905, Perplexity:  4.02\n","Epoch [178/200], Loss: 1.3893, Perplexity:  4.01\n","Epoch [179/200], Loss: 1.3890, Perplexity:  4.01\n","Epoch [180/200], Loss: 1.3853, Perplexity:  4.00\n","Epoch [181/200], Loss: 1.3780, Perplexity:  3.97\n","Epoch [182/200], Loss: 1.3824, Perplexity:  3.98\n","Epoch [183/200], Loss: 1.3796, Perplexity:  3.97\n","Epoch [184/200], Loss: 1.3761, Perplexity:  3.96\n","Epoch [185/200], Loss: 1.3728, Perplexity:  3.95\n","Epoch [186/200], Loss: 1.3757, Perplexity:  3.96\n","Epoch [187/200], Loss: 1.3746, Perplexity:  3.95\n","Epoch [188/200], Loss: 1.3707, Perplexity:  3.94\n","Epoch [189/200], Loss: 1.3686, Perplexity:  3.93\n","Epoch [190/200], Loss: 1.3712, Perplexity:  3.94\n","Epoch [191/200], Loss: 1.3666, Perplexity:  3.92\n","Epoch [192/200], Loss: 1.3620, Perplexity:  3.90\n","Epoch [193/200], Loss: 1.3648, Perplexity:  3.91\n","Epoch [194/200], Loss: 1.3599, Perplexity:  3.90\n","Epoch [195/200], Loss: 1.3573, Perplexity:  3.89\n","Epoch [196/200], Loss: 1.3602, Perplexity:  3.90\n","Epoch [197/200], Loss: 1.3580, Perplexity:  3.89\n","Epoch [198/200], Loss: 1.3508, Perplexity:  3.86\n","Epoch [199/200], Loss: 1.3497, Perplexity:  3.86\n","Epoch [200/200], Loss: 1.3565, Perplexity:  3.88\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KUPNfDYqVVOM","colab_type":"code","colab":{}},"source":["import pandas as pd\n","\n","frame = pd.DataFrame(zip(train_loss_lstm_list,train_perp_lstm_list))\n","frame.columns = ['train_loss','train_perp']\n","frame.to_csv('1_LSTM(Adam).csv')\n","\n","# plt.figure(figsize=(10,6))\n","# plt.title('SGD default lr, 200 epochs, reduce lr on plateau on 150 epoch')\n","# plt.plot(range(3),frame['train_loss'])\n","\n","\n","# plt.figure(figsize=(10,6))\n","# plt.title('SGD default lr, 200 epochs, reduce lr on plateau on 150 epoch')\n","# plt.plot(range(3),frame['train_perp'])"],"execution_count":0,"outputs":[]}]}