{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Penn_Treebank_1_LSTM(AdaGrad).ipynb","provenance":[{"file_id":"1PVTmJ94PgoIHVW1Q315a7Xof_wuf05Oq","timestamp":1590442803558},{"file_id":"1C7tl3uUJ2eh-A39b93b0JPx8qy8RuYdN","timestamp":1590433263405},{"file_id":"18_0qHuK5tvwAfyVVoruWVta1pxBzDQxC","timestamp":1590419745097}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"S6lyfG8jZt1b","colab_type":"text"},"source":["# Init"]},{"cell_type":"code","metadata":{"id":"usrX2f-qZvi5","colab_type":"code","outputId":"e5e75c5a-1309-4188-bbe1-343e37e1cde0","executionInfo":{"status":"ok","timestamp":1590442917211,"user_tz":-60,"elapsed":7641,"user":{"displayName":"Eric Lee","photoUrl":"","userId":"15439544951905613244"}},"colab":{"base_uri":"https://localhost:8080/","height":168}},"source":["# Execute this code block to install dependencies when running on colab\n","try:\n","    import torch\n","except:\n","    from os.path import exists\n","    from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n","    platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n","    cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n","    accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n","\n","    !pip install -q http://download.pytorch.org/whl/{accelerator}/torch-1.0.0-{platform}-linux_x86_64.whl torchvision\n","\n","try: \n","    import torchbearer\n","except:\n","    !pip install torchbearer"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting torchbearer\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/e9/4049a47dd2e5b6346a2c5d215b0c67dce814afbab1cd54ce024533c4834e/torchbearer-0.5.3-py3-none-any.whl (138kB)\n","\r\u001b[K     |██▍                             | 10kB 24.8MB/s eta 0:00:01\r\u001b[K     |████▊                           | 20kB 3.1MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 30kB 4.1MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 40kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 51kB 3.7MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 61kB 4.1MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 71kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 81kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 92kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 102kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 112kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 122kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 133kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 143kB 5.0MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchbearer) (4.41.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchbearer) (1.18.4)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from torchbearer) (1.5.0+cu101)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->torchbearer) (0.16.0)\n","Installing collected packages: torchbearer\n","Successfully installed torchbearer-0.5.3\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RQV9LbVMZ5NG","colab_type":"text"},"source":["# Prepare Penn Treebank dataset"]},{"cell_type":"code","metadata":{"id":"KhEeh6x8Z9eE","colab_type":"code","colab":{}},"source":["# automatically reload external modules if they change\n","%load_ext autoreload\n","%autoreload 2\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torchtext import data\n","from torchtext import vocab\n","from torchtext import datasets\n","\n","import numpy as np\n","from matplotlib import pyplot as plt\n","\n","from tqdm import tqdm"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NWmejPqvd93t","colab_type":"code","outputId":"5a49e432-4ec8-4b88-bb7d-9c208c78e50f","executionInfo":{"status":"ok","timestamp":1590443374647,"user_tz":-60,"elapsed":465062,"user":{"displayName":"Eric Lee","photoUrl":"","userId":"15439544951905613244"}},"colab":{"base_uri":"https://localhost:8080/","height":168}},"source":["tokenize = lambda x: x.split()\n","TEXT = data.Field(sequential = True, tokenize = tokenize, lower = True, batch_first = True)\n","train_dataset, val_dataset, test_dataset = datasets.PennTreebank.splits(TEXT)\n","TEXT.build_vocab(train_dataset, vectors=vocab.GloVe(name='6B', dim=300))\n","\n","vocab_size = len(TEXT.vocab)\n","word_embeddings = TEXT.vocab.vectors\n","print(vocab_size)\n","print(word_embeddings.size())\n","embeddings_length = 300\n","hidden_size = 256\n","batch_size = 32"],"execution_count":0,"outputs":[{"output_type":"stream","text":["downloading ptb.train.txt\n"],"name":"stdout"},{"output_type":"stream","text":["ptb.train.txt: 5.10MB [00:00, 44.5MB/s]\n"],"name":"stderr"},{"output_type":"stream","text":["downloading ptb.valid.txt\n"],"name":"stdout"},{"output_type":"stream","text":["ptb.valid.txt: 400kB [00:00, 17.9MB/s]                   \n"],"name":"stderr"},{"output_type":"stream","text":["downloading ptb.test.txt\n"],"name":"stdout"},{"output_type":"stream","text":["ptb.test.txt: 450kB [00:00, 19.6MB/s]                   \n",".vector_cache/glove.6B.zip: 862MB [06:26, 2.23MB/s]                           \n","100%|█████████▉| 399325/400000 [00:36<00:00, 10781.76it/s]"],"name":"stderr"},{"output_type":"stream","text":["10001\n","torch.Size([10001, 300])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5Hl-iXexfSWl","colab_type":"code","colab":{}},"source":["train_iter, val_iter, test_iter = data.BPTTIterator.splits((train_dataset, val_dataset, test_dataset), batch_size = 32, bptt_len=30, repeat=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wPLZIYqKhvTF","colab_type":"code","colab":{}},"source":["class LstmLangModel(nn.Module):\n","   def __init__(self, batch_size, hidden_size, vocab_size, embeddings_length, weights):\n","       super(LstmLangModel, self).__init__()\n","       self.batch_size = batch_size\n","       self.hidden_size = hidden_size\n","       self.vocab_size = vocab_size\n","       self.embed = nn.Embedding(vocab_size, embeddings_length)\n","       self.embed.weight.data.copy_(weights)\n","       self.lstm = nn.LSTM(embeddings_length, hidden_size, batch_first=True)\n","       self.fc = nn.Linear(hidden_size, vocab_size)\n","   def forward(self, x, h):\n","       x = self.embed(x)\n","       output_seq, (h, c) = self.lstm(x, h)\n","       out = output_seq.reshape(output_seq.size(0)*output_seq.size(1), output_seq.size(2))\n","       out = self.fc(out)\n","       return out, (h, c)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ySgb_BtfkbjZ","colab_type":"code","outputId":"38ee2547-97a8-4145-889c-b6327fb45557","executionInfo":{"status":"ok","timestamp":1590443374650,"user_tz":-60,"elapsed":465054,"user":{"displayName":"Eric Lee","photoUrl":"","userId":"15439544951905613244"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["vocab_size = len(TEXT.vocab)\n","word_embeddings = TEXT.vocab.vectors\n","print(vocab_size)\n","print(word_embeddings.size())\n","embeddings_length = 300\n","hidden_size = 256\n","batch_size = 32"],"execution_count":0,"outputs":[{"output_type":"stream","text":["10001\n","torch.Size([10001, 300])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"O7h9zfUYkFEc","colab_type":"code","outputId":"74fb6c9d-2c7b-498e-d31c-86f52cd5b7ae","executionInfo":{"status":"ok","timestamp":1590443374930,"user_tz":-60,"elapsed":465327,"user":{"displayName":"Eric Lee","photoUrl":"","userId":"15439544951905613244"}},"colab":{"base_uri":"https://localhost:8080/","height":101}},"source":["model = LstmLangModel(batch_size, hidden_size, vocab_size, embeddings_length, word_embeddings)\n","model.eval()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LstmLangModel(\n","  (embed): Embedding(10001, 300)\n","  (lstm): LSTM(300, 256, batch_first=True)\n","  (fc): Linear(in_features=256, out_features=10001, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"k6CWQqenmQ56","colab_type":"text"},"source":["# Torchbear Section"]},{"cell_type":"code","metadata":{"id":"TtOu1hMbjzbZ","colab_type":"code","outputId":"fbfd4daf-8935-40ee-e557-96cf587622d5","executionInfo":{"status":"ok","timestamp":1590444810938,"user_tz":-60,"elapsed":1901328,"user":{"displayName":"Eric Lee","photoUrl":"","userId":"15439544951905613244"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","net_lstm = LstmLangModel(batch_size, hidden_size, vocab_size, embeddings_length, word_embeddings)\n","net_lstm = net_lstm.to(device)\n","criterion = nn.CrossEntropyLoss()\n","optim = torch.optim.Adagrad(filter(lambda p: p.requires_grad, net_lstm.parameters()),lr=0.01)\n","num_epochs = 200\n","epoch_list = []\n","train_loss_lstm_list = []\n","train_perp_lstm_list = []\n","\n","def detach(states):\n","   return [state.detach() for state in states]\n","\n","\n","\n","for epoch in range(num_epochs):\n","   train_loss = 0\n","   states = (torch.zeros(1, batch_size, hidden_size).to(device),\n","             torch.zeros(1, batch_size, hidden_size).to(device))\n","   net_lstm.train()\n","\n","   for i, batch in enumerate(train_iter):\n","       text = batch.text.to(device)\n","       labels = batch.target.to(device)\n","       text = text.permute(1, 0)\n","       labels = labels.permute(1, 0)\n","\n","       optim.zero_grad()\n","       states = detach(states)\n","       outputs, states = net_lstm(text, states)\n","       loss = criterion(outputs, labels.reshape(-1))\n","       train_loss += loss.item()\n","       loss.backward()\n","       optim.step()\n","   avg_train_loss = train_loss / len(train_iter)\n","   perplexity = np.exp(avg_train_loss)\n","   print('Epoch [{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'.format(epoch + 1, num_epochs, avg_train_loss, perplexity))\n","   train_loss_lstm_list.append(avg_train_loss)\n","   train_perp_lstm_list.append(perplexity)\n","\n","   if epoch % 100 == 0:\n","       torch.save(net_lstm.state_dict(), r\"./LSTM_\" + str(epoch) + r\".pth\")\n","   \n","   if epoch == num_epochs - 1:\n","       torch.save(net_lstm.state_dict(), r\"./LSTM_\" + str(epoch) + r\".pth\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\r100%|█████████▉| 399325/400000 [00:50<00:00, 10781.76it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch [1/200], Loss: 5.5091, Perplexity: 246.92\n","Epoch [2/200], Loss: 5.0245, Perplexity: 152.10\n","Epoch [3/200], Loss: 4.8469, Perplexity: 127.35\n","Epoch [4/200], Loss: 4.7299, Perplexity: 113.28\n","Epoch [5/200], Loss: 4.6417, Perplexity: 103.72\n","Epoch [6/200], Loss: 4.5708, Perplexity: 96.62\n","Epoch [7/200], Loss: 4.5115, Perplexity: 91.06\n","Epoch [8/200], Loss: 4.4604, Perplexity: 86.52\n","Epoch [9/200], Loss: 4.4154, Perplexity: 82.71\n","Epoch [10/200], Loss: 4.3752, Perplexity: 79.46\n","Epoch [11/200], Loss: 4.3388, Perplexity: 76.61\n","Epoch [12/200], Loss: 4.3055, Perplexity: 74.10\n","Epoch [13/200], Loss: 4.2747, Perplexity: 71.86\n","Epoch [14/200], Loss: 4.2461, Perplexity: 69.84\n","Epoch [15/200], Loss: 4.2194, Perplexity: 68.00\n","Epoch [16/200], Loss: 4.1943, Perplexity: 66.31\n","Epoch [17/200], Loss: 4.1707, Perplexity: 64.76\n","Epoch [18/200], Loss: 4.1482, Perplexity: 63.32\n","Epoch [19/200], Loss: 4.1269, Perplexity: 61.99\n","Epoch [20/200], Loss: 4.1066, Perplexity: 60.74\n","Epoch [21/200], Loss: 4.0871, Perplexity: 59.57\n","Epoch [22/200], Loss: 4.0685, Perplexity: 58.47\n","Epoch [23/200], Loss: 4.0506, Perplexity: 57.43\n","Epoch [24/200], Loss: 4.0333, Perplexity: 56.45\n","Epoch [25/200], Loss: 4.0167, Perplexity: 55.52\n","Epoch [26/200], Loss: 4.0006, Perplexity: 54.63\n","Epoch [27/200], Loss: 3.9850, Perplexity: 53.79\n","Epoch [28/200], Loss: 3.9699, Perplexity: 52.98\n","Epoch [29/200], Loss: 3.9553, Perplexity: 52.21\n","Epoch [30/200], Loss: 3.9410, Perplexity: 51.47\n","Epoch [31/200], Loss: 3.9272, Perplexity: 50.76\n","Epoch [32/200], Loss: 3.9137, Perplexity: 50.08\n","Epoch [33/200], Loss: 3.9006, Perplexity: 49.43\n","Epoch [34/200], Loss: 3.8878, Perplexity: 48.80\n","Epoch [35/200], Loss: 3.8752, Perplexity: 48.19\n","Epoch [36/200], Loss: 3.8630, Perplexity: 47.61\n","Epoch [37/200], Loss: 3.8511, Perplexity: 47.04\n","Epoch [38/200], Loss: 3.8394, Perplexity: 46.50\n","Epoch [39/200], Loss: 3.8279, Perplexity: 45.97\n","Epoch [40/200], Loss: 3.8166, Perplexity: 45.45\n","Epoch [41/200], Loss: 3.8056, Perplexity: 44.95\n","Epoch [42/200], Loss: 3.7948, Perplexity: 44.47\n","Epoch [43/200], Loss: 3.7842, Perplexity: 44.00\n","Epoch [44/200], Loss: 3.7738, Perplexity: 43.55\n","Epoch [45/200], Loss: 3.7636, Perplexity: 43.10\n","Epoch [46/200], Loss: 3.7535, Perplexity: 42.67\n","Epoch [47/200], Loss: 3.7436, Perplexity: 42.25\n","Epoch [48/200], Loss: 3.7339, Perplexity: 41.84\n","Epoch [49/200], Loss: 3.7244, Perplexity: 41.44\n","Epoch [50/200], Loss: 3.7149, Perplexity: 41.06\n","Epoch [51/200], Loss: 3.7056, Perplexity: 40.68\n","Epoch [52/200], Loss: 3.6965, Perplexity: 40.31\n","Epoch [53/200], Loss: 3.6875, Perplexity: 39.94\n","Epoch [54/200], Loss: 3.6786, Perplexity: 39.59\n","Epoch [55/200], Loss: 3.6699, Perplexity: 39.25\n","Epoch [56/200], Loss: 3.6612, Perplexity: 38.91\n","Epoch [57/200], Loss: 3.6527, Perplexity: 38.58\n","Epoch [58/200], Loss: 3.6443, Perplexity: 38.26\n","Epoch [59/200], Loss: 3.6360, Perplexity: 37.94\n","Epoch [60/200], Loss: 3.6278, Perplexity: 37.63\n","Epoch [61/200], Loss: 3.6197, Perplexity: 37.33\n","Epoch [62/200], Loss: 3.6117, Perplexity: 37.03\n","Epoch [63/200], Loss: 3.6038, Perplexity: 36.74\n","Epoch [64/200], Loss: 3.5960, Perplexity: 36.45\n","Epoch [65/200], Loss: 3.5883, Perplexity: 36.17\n","Epoch [66/200], Loss: 3.5806, Perplexity: 35.90\n","Epoch [67/200], Loss: 3.5731, Perplexity: 35.63\n","Epoch [68/200], Loss: 3.5656, Perplexity: 35.36\n","Epoch [69/200], Loss: 3.5583, Perplexity: 35.10\n","Epoch [70/200], Loss: 3.5509, Perplexity: 34.85\n","Epoch [71/200], Loss: 3.5437, Perplexity: 34.59\n","Epoch [72/200], Loss: 3.5365, Perplexity: 34.35\n","Epoch [73/200], Loss: 3.5294, Perplexity: 34.11\n","Epoch [74/200], Loss: 3.5224, Perplexity: 33.87\n","Epoch [75/200], Loss: 3.5155, Perplexity: 33.63\n","Epoch [76/200], Loss: 3.5086, Perplexity: 33.40\n","Epoch [77/200], Loss: 3.5018, Perplexity: 33.17\n","Epoch [78/200], Loss: 3.4950, Perplexity: 32.95\n","Epoch [79/200], Loss: 3.4883, Perplexity: 32.73\n","Epoch [80/200], Loss: 3.4817, Perplexity: 32.51\n","Epoch [81/200], Loss: 3.4751, Perplexity: 32.30\n","Epoch [82/200], Loss: 3.4686, Perplexity: 32.09\n","Epoch [83/200], Loss: 3.4621, Perplexity: 31.88\n","Epoch [84/200], Loss: 3.4557, Perplexity: 31.68\n","Epoch [85/200], Loss: 3.4494, Perplexity: 31.48\n","Epoch [86/200], Loss: 3.4431, Perplexity: 31.28\n","Epoch [87/200], Loss: 3.4369, Perplexity: 31.09\n","Epoch [88/200], Loss: 3.4307, Perplexity: 30.90\n","Epoch [89/200], Loss: 3.4245, Perplexity: 30.71\n","Epoch [90/200], Loss: 3.4184, Perplexity: 30.52\n","Epoch [91/200], Loss: 3.4124, Perplexity: 30.34\n","Epoch [92/200], Loss: 3.4064, Perplexity: 30.16\n","Epoch [93/200], Loss: 3.4005, Perplexity: 29.98\n","Epoch [94/200], Loss: 3.3946, Perplexity: 29.80\n","Epoch [95/200], Loss: 3.3887, Perplexity: 29.63\n","Epoch [96/200], Loss: 3.3829, Perplexity: 29.46\n","Epoch [97/200], Loss: 3.3771, Perplexity: 29.29\n","Epoch [98/200], Loss: 3.3714, Perplexity: 29.12\n","Epoch [99/200], Loss: 3.3657, Perplexity: 28.95\n","Epoch [100/200], Loss: 3.3601, Perplexity: 28.79\n","Epoch [101/200], Loss: 3.3545, Perplexity: 28.63\n","Epoch [102/200], Loss: 3.3489, Perplexity: 28.47\n","Epoch [103/200], Loss: 3.3434, Perplexity: 28.32\n","Epoch [104/200], Loss: 3.3379, Perplexity: 28.16\n","Epoch [105/200], Loss: 3.3325, Perplexity: 28.01\n","Epoch [106/200], Loss: 3.3271, Perplexity: 27.86\n","Epoch [107/200], Loss: 3.3217, Perplexity: 27.71\n","Epoch [108/200], Loss: 3.3164, Perplexity: 27.56\n","Epoch [109/200], Loss: 3.3111, Perplexity: 27.42\n","Epoch [110/200], Loss: 3.3059, Perplexity: 27.27\n","Epoch [111/200], Loss: 3.3006, Perplexity: 27.13\n","Epoch [112/200], Loss: 3.2955, Perplexity: 26.99\n","Epoch [113/200], Loss: 3.2903, Perplexity: 26.85\n","Epoch [114/200], Loss: 3.2852, Perplexity: 26.71\n","Epoch [115/200], Loss: 3.2800, Perplexity: 26.58\n","Epoch [116/200], Loss: 3.2750, Perplexity: 26.44\n","Epoch [117/200], Loss: 3.2700, Perplexity: 26.31\n","Epoch [118/200], Loss: 3.2652, Perplexity: 26.18\n","Epoch [119/200], Loss: 3.2601, Perplexity: 26.05\n","Epoch [120/200], Loss: 3.2551, Perplexity: 25.92\n","Epoch [121/200], Loss: 3.2502, Perplexity: 25.80\n","Epoch [122/200], Loss: 3.2453, Perplexity: 25.67\n","Epoch [123/200], Loss: 3.2405, Perplexity: 25.55\n","Epoch [124/200], Loss: 3.2357, Perplexity: 25.42\n","Epoch [125/200], Loss: 3.2309, Perplexity: 25.30\n","Epoch [126/200], Loss: 3.2262, Perplexity: 25.18\n","Epoch [127/200], Loss: 3.2214, Perplexity: 25.06\n","Epoch [128/200], Loss: 3.2167, Perplexity: 24.95\n","Epoch [129/200], Loss: 3.2120, Perplexity: 24.83\n","Epoch [130/200], Loss: 3.2074, Perplexity: 24.71\n","Epoch [131/200], Loss: 3.2028, Perplexity: 24.60\n","Epoch [132/200], Loss: 3.1982, Perplexity: 24.49\n","Epoch [133/200], Loss: 3.1937, Perplexity: 24.38\n","Epoch [134/200], Loss: 3.1891, Perplexity: 24.27\n","Epoch [135/200], Loss: 3.1845, Perplexity: 24.16\n","Epoch [136/200], Loss: 3.1800, Perplexity: 24.05\n","Epoch [137/200], Loss: 3.1756, Perplexity: 23.94\n","Epoch [138/200], Loss: 3.1715, Perplexity: 23.84\n","Epoch [139/200], Loss: 3.1669, Perplexity: 23.73\n","Epoch [140/200], Loss: 3.1623, Perplexity: 23.63\n","Epoch [141/200], Loss: 3.1580, Perplexity: 23.52\n","Epoch [142/200], Loss: 3.1536, Perplexity: 23.42\n","Epoch [143/200], Loss: 3.1493, Perplexity: 23.32\n","Epoch [144/200], Loss: 3.1450, Perplexity: 23.22\n","Epoch [145/200], Loss: 3.1407, Perplexity: 23.12\n","Epoch [146/200], Loss: 3.1365, Perplexity: 23.02\n","Epoch [147/200], Loss: 3.1323, Perplexity: 22.93\n","Epoch [148/200], Loss: 3.1280, Perplexity: 22.83\n","Epoch [149/200], Loss: 3.1238, Perplexity: 22.73\n","Epoch [150/200], Loss: 3.1196, Perplexity: 22.64\n","Epoch [151/200], Loss: 3.1155, Perplexity: 22.54\n","Epoch [152/200], Loss: 3.1114, Perplexity: 22.45\n","Epoch [153/200], Loss: 3.1073, Perplexity: 22.36\n","Epoch [154/200], Loss: 3.1032, Perplexity: 22.27\n","Epoch [155/200], Loss: 3.0991, Perplexity: 22.18\n","Epoch [156/200], Loss: 3.0951, Perplexity: 22.09\n","Epoch [157/200], Loss: 3.0911, Perplexity: 22.00\n","Epoch [158/200], Loss: 3.0869, Perplexity: 21.91\n","Epoch [159/200], Loss: 3.0830, Perplexity: 21.82\n","Epoch [160/200], Loss: 3.0792, Perplexity: 21.74\n","Epoch [161/200], Loss: 3.0752, Perplexity: 21.65\n","Epoch [162/200], Loss: 3.0712, Perplexity: 21.57\n","Epoch [163/200], Loss: 3.0672, Perplexity: 21.48\n","Epoch [164/200], Loss: 3.0634, Perplexity: 21.40\n","Epoch [165/200], Loss: 3.0595, Perplexity: 21.32\n","Epoch [166/200], Loss: 3.0556, Perplexity: 21.23\n","Epoch [167/200], Loss: 3.0518, Perplexity: 21.15\n","Epoch [168/200], Loss: 3.0480, Perplexity: 21.07\n","Epoch [169/200], Loss: 3.0441, Perplexity: 20.99\n","Epoch [170/200], Loss: 3.0403, Perplexity: 20.91\n","Epoch [171/200], Loss: 3.0367, Perplexity: 20.84\n","Epoch [172/200], Loss: 3.0329, Perplexity: 20.76\n","Epoch [173/200], Loss: 3.0293, Perplexity: 20.68\n","Epoch [174/200], Loss: 3.0254, Perplexity: 20.60\n","Epoch [175/200], Loss: 3.0217, Perplexity: 20.53\n","Epoch [176/200], Loss: 3.0182, Perplexity: 20.45\n","Epoch [177/200], Loss: 3.0145, Perplexity: 20.38\n","Epoch [178/200], Loss: 3.0109, Perplexity: 20.31\n","Epoch [179/200], Loss: 3.0073, Perplexity: 20.23\n","Epoch [180/200], Loss: 3.0036, Perplexity: 20.16\n","Epoch [181/200], Loss: 3.0002, Perplexity: 20.09\n","Epoch [182/200], Loss: 2.9965, Perplexity: 20.02\n","Epoch [183/200], Loss: 2.9929, Perplexity: 19.94\n","Epoch [184/200], Loss: 2.9894, Perplexity: 19.87\n","Epoch [185/200], Loss: 2.9859, Perplexity: 19.80\n","Epoch [186/200], Loss: 2.9824, Perplexity: 19.73\n","Epoch [187/200], Loss: 2.9789, Perplexity: 19.67\n","Epoch [188/200], Loss: 2.9752, Perplexity: 19.59\n","Epoch [189/200], Loss: 2.9720, Perplexity: 19.53\n","Epoch [190/200], Loss: 2.9687, Perplexity: 19.47\n","Epoch [191/200], Loss: 2.9650, Perplexity: 19.39\n","Epoch [192/200], Loss: 2.9616, Perplexity: 19.33\n","Epoch [193/200], Loss: 2.9584, Perplexity: 19.27\n","Epoch [194/200], Loss: 2.9549, Perplexity: 19.20\n","Epoch [195/200], Loss: 2.9515, Perplexity: 19.14\n","Epoch [196/200], Loss: 2.9484, Perplexity: 19.08\n","Epoch [197/200], Loss: 2.9450, Perplexity: 19.01\n","Epoch [198/200], Loss: 2.9417, Perplexity: 18.95\n","Epoch [199/200], Loss: 2.9408, Perplexity: 18.93\n","Epoch [200/200], Loss: 2.9369, Perplexity: 18.86\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KUPNfDYqVVOM","colab_type":"code","colab":{}},"source":["import pandas as pd\n","\n","frame = pd.DataFrame(zip(train_loss_lstm_list,train_perp_lstm_list))\n","frame.columns = ['train_loss','train_perp']\n","frame.to_csv('1_LSTM(Adagrad).csv')\n","\n","# plt.figure(figsize=(10,6))\n","# plt.title('SGD default lr, 200 epochs, reduce lr on plateau on 150 epoch')\n","# plt.plot(range(3),frame['train_loss'])\n","\n","\n","# plt.figure(figsize=(10,6))\n","# plt.title('SGD default lr, 200 epochs, reduce lr on plateau on 150 epoch')\n","# plt.plot(range(3),frame['train_perp'])"],"execution_count":0,"outputs":[]}]}