{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Penn_Treebank_1_LSTM(AMSBound).ipynb","provenance":[{"file_id":"1iOUtjw-ijA-ubfzIv-HnBcqE6HE2_3JI","timestamp":1590445016928},{"file_id":"1PVTmJ94PgoIHVW1Q315a7Xof_wuf05Oq","timestamp":1590442803558},{"file_id":"1C7tl3uUJ2eh-A39b93b0JPx8qy8RuYdN","timestamp":1590433263405},{"file_id":"18_0qHuK5tvwAfyVVoruWVta1pxBzDQxC","timestamp":1590419745097}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"S6lyfG8jZt1b","colab_type":"text"},"source":["# Init"]},{"cell_type":"code","metadata":{"id":"usrX2f-qZvi5","colab_type":"code","colab":{}},"source":["# Execute this code block to install dependencies when running on colab\n","try:\n","    import torch\n","except:\n","    from os.path import exists\n","    from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n","    platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n","    cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n","    accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n","\n","    !pip install -q http://download.pytorch.org/whl/{accelerator}/torch-1.0.0-{platform}-linux_x86_64.whl torchvision\n","\n","try: \n","    import torchbearer\n","except:\n","    !pip install torchbearer"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RQV9LbVMZ5NG","colab_type":"text"},"source":["# Prepare Penn Treebank dataset"]},{"cell_type":"code","metadata":{"id":"KhEeh6x8Z9eE","colab_type":"code","outputId":"4f506e92-a0bc-4e6c-c275-5eca1fd2315f","executionInfo":{"status":"ok","timestamp":1590449344057,"user_tz":-60,"elapsed":1183,"user":{"displayName":"Eric Lee","photoUrl":"","userId":"15439544951905613244"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["# automatically reload external modules if they change\n","%load_ext autoreload\n","%autoreload 2\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torchtext import data\n","from torchtext import vocab\n","from torchtext import datasets\n","\n","import numpy as np\n","from matplotlib import pyplot as plt\n","\n","from tqdm import tqdm"],"execution_count":0,"outputs":[{"output_type":"stream","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IeNzwr1P6Zpy","colab_type":"code","outputId":"d5f34282-02c0-4a69-8ce3-3ce3f99592d6","executionInfo":{"status":"ok","timestamp":1590449346633,"user_tz":-60,"elapsed":3749,"user":{"displayName":"Eric Lee","photoUrl":"","userId":"15439544951905613244"}},"colab":{"base_uri":"https://localhost:8080/","height":84}},"source":["!pip install adabound"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: adabound in /usr/local/lib/python3.6/dist-packages (0.0.5)\n","Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from adabound) (1.5.0+cu101)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.0->adabound) (0.16.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.0->adabound) (1.18.4)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XZvRJcCM6e0h","colab_type":"code","colab":{}},"source":["import adabound"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NWmejPqvd93t","colab_type":"code","outputId":"7dc1936a-e4f3-49d3-93d5-b17f9d248ba8","executionInfo":{"status":"ok","timestamp":1590449348594,"user_tz":-60,"elapsed":5698,"user":{"displayName":"Eric Lee","photoUrl":"","userId":"15439544951905613244"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["tokenize = lambda x: x.split()\n","TEXT = data.Field(sequential = True, tokenize = tokenize, lower = True, batch_first = True)\n","train_dataset, val_dataset, test_dataset = datasets.PennTreebank.splits(TEXT)\n","TEXT.build_vocab(train_dataset, vectors=vocab.GloVe(name='6B', dim=300))\n","\n","vocab_size = len(TEXT.vocab)\n","word_embeddings = TEXT.vocab.vectors\n","print(vocab_size)\n","print(word_embeddings.size())\n","embeddings_length = 300\n","hidden_size = 256\n","batch_size = 32"],"execution_count":0,"outputs":[{"output_type":"stream","text":["10001\n","torch.Size([10001, 300])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5Hl-iXexfSWl","colab_type":"code","colab":{}},"source":["train_iter, val_iter, test_iter = data.BPTTIterator.splits((train_dataset, val_dataset, test_dataset), batch_size = 32, bptt_len=30, repeat=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wPLZIYqKhvTF","colab_type":"code","colab":{}},"source":["class LstmLangModel(nn.Module):\n","   def __init__(self, batch_size, hidden_size, vocab_size, embeddings_length, weights):\n","       super(LstmLangModel, self).__init__()\n","       self.batch_size = batch_size\n","       self.hidden_size = hidden_size\n","       self.vocab_size = vocab_size\n","       self.embed = nn.Embedding(vocab_size, embeddings_length)\n","       self.embed.weight.data.copy_(weights)\n","       self.lstm = nn.LSTM(embeddings_length, hidden_size, batch_first=True)\n","       self.fc = nn.Linear(hidden_size, vocab_size)\n","   def forward(self, x, h):\n","       x = self.embed(x)\n","       output_seq, (h, c) = self.lstm(x, h)\n","       out = output_seq.reshape(output_seq.size(0)*output_seq.size(1), output_seq.size(2))\n","       out = self.fc(out)\n","       return out, (h, c)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ySgb_BtfkbjZ","colab_type":"code","outputId":"cc91e25c-2605-4b62-85a3-2144afc35652","executionInfo":{"status":"ok","timestamp":1590449348595,"user_tz":-60,"elapsed":5683,"user":{"displayName":"Eric Lee","photoUrl":"","userId":"15439544951905613244"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["vocab_size = len(TEXT.vocab)\n","word_embeddings = TEXT.vocab.vectors\n","print(vocab_size)\n","print(word_embeddings.size())\n","embeddings_length = 300\n","hidden_size = 256\n","batch_size = 32"],"execution_count":0,"outputs":[{"output_type":"stream","text":["10001\n","torch.Size([10001, 300])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"O7h9zfUYkFEc","colab_type":"code","outputId":"1a16ba87-52be-4796-e022-520147f8cb17","executionInfo":{"status":"ok","timestamp":1590449348596,"user_tz":-60,"elapsed":5671,"user":{"displayName":"Eric Lee","photoUrl":"","userId":"15439544951905613244"}},"colab":{"base_uri":"https://localhost:8080/","height":101}},"source":["model = LstmLangModel(batch_size, hidden_size, vocab_size, embeddings_length, word_embeddings)\n","model.eval()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LstmLangModel(\n","  (embed): Embedding(10001, 300)\n","  (lstm): LSTM(300, 256, batch_first=True)\n","  (fc): Linear(in_features=256, out_features=10001, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"k6CWQqenmQ56","colab_type":"text"},"source":["# Torchbear Section"]},{"cell_type":"code","metadata":{"id":"TtOu1hMbjzbZ","colab_type":"code","outputId":"06f1c8bf-6158-4ce0-efad-98697dd68f9c","executionInfo":{"status":"ok","timestamp":1590452541413,"user_tz":-60,"elapsed":3198476,"user":{"displayName":"Eric Lee","photoUrl":"","userId":"15439544951905613244"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","net_lstm = LstmLangModel(batch_size, hidden_size, vocab_size, embeddings_length, word_embeddings)\n","net_lstm = net_lstm.to(device)\n","criterion = nn.CrossEntropyLoss()\n","optim = adabound.AdaBound(filter(lambda p: p.requires_grad, net_lstm.parameters()),lr=1e-3, final_lr=0.1,gamma = 0.001,amsbound = True)\n","num_epochs = 200\n","epoch_list = []\n","train_loss_lstm_list = []\n","train_perp_lstm_list = []\n","\n","def detach(states):\n","   return [state.detach() for state in states]\n","\n","\n","\n","for epoch in range(num_epochs):\n","   train_loss = 0\n","   states = (torch.zeros(1, batch_size, hidden_size).to(device),\n","             torch.zeros(1, batch_size, hidden_size).to(device))\n","   net_lstm.train()\n","\n","   for i, batch in enumerate(train_iter):\n","       text = batch.text.to(device)\n","       labels = batch.target.to(device)\n","       text = text.permute(1, 0)\n","       labels = labels.permute(1, 0)\n","\n","       optim.zero_grad()\n","       states = detach(states)\n","       outputs, states = net_lstm(text, states)\n","       loss = criterion(outputs, labels.reshape(-1))\n","       train_loss += loss.item()\n","       loss.backward()\n","       optim.step()\n","   avg_train_loss = train_loss / len(train_iter)\n","   perplexity = np.exp(avg_train_loss)\n","   print('Epoch [{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'.format(epoch + 1, num_epochs, avg_train_loss, perplexity))\n","   train_loss_lstm_list.append(avg_train_loss)\n","   train_perp_lstm_list.append(perplexity)\n","\n","   if epoch % 100 == 0:\n","       torch.save(net_lstm.state_dict(), r\"./LSTM_\" + str(epoch) + r\".pth\")\n","   \n","   if epoch == num_epochs - 1:\n","       torch.save(net_lstm.state_dict(), r\"./LSTM_\" + str(epoch) + r\".pth\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch [1/200], Loss: 6.4552, Perplexity: 636.00\n","Epoch [2/200], Loss: 6.2394, Perplexity: 512.54\n","Epoch [3/200], Loss: 6.1680, Perplexity: 477.23\n","Epoch [4/200], Loss: 6.1192, Perplexity: 454.50\n","Epoch [5/200], Loss: 6.0798, Perplexity: 436.94\n","Epoch [6/200], Loss: 6.0454, Perplexity: 422.18\n","Epoch [7/200], Loss: 6.0145, Perplexity: 409.30\n","Epoch [8/200], Loss: 5.9860, Perplexity: 397.81\n","Epoch [9/200], Loss: 5.9595, Perplexity: 387.40\n","Epoch [10/200], Loss: 5.9344, Perplexity: 377.80\n","Epoch [11/200], Loss: 5.9106, Perplexity: 368.92\n","Epoch [12/200], Loss: 5.8879, Perplexity: 360.66\n","Epoch [13/200], Loss: 5.8664, Perplexity: 352.96\n","Epoch [14/200], Loss: 5.8456, Perplexity: 345.73\n","Epoch [15/200], Loss: 5.8258, Perplexity: 338.92\n","Epoch [16/200], Loss: 5.8065, Perplexity: 332.45\n","Epoch [17/200], Loss: 5.7874, Perplexity: 326.18\n","Epoch [18/200], Loss: 5.7685, Perplexity: 320.07\n","Epoch [19/200], Loss: 5.7503, Perplexity: 314.30\n","Epoch [20/200], Loss: 5.7329, Perplexity: 308.87\n","Epoch [21/200], Loss: 5.7164, Perplexity: 303.80\n","Epoch [22/200], Loss: 5.7005, Perplexity: 299.00\n","Epoch [23/200], Loss: 5.6851, Perplexity: 294.45\n","Epoch [24/200], Loss: 5.6703, Perplexity: 290.12\n","Epoch [25/200], Loss: 5.6560, Perplexity: 285.99\n","Epoch [26/200], Loss: 5.6421, Perplexity: 282.05\n","Epoch [27/200], Loss: 5.6285, Perplexity: 278.26\n","Epoch [28/200], Loss: 5.6154, Perplexity: 274.61\n","Epoch [29/200], Loss: 5.6025, Perplexity: 271.10\n","Epoch [30/200], Loss: 5.5899, Perplexity: 267.71\n","Epoch [31/200], Loss: 5.5776, Perplexity: 264.43\n","Epoch [32/200], Loss: 5.5655, Perplexity: 261.27\n","Epoch [33/200], Loss: 5.5537, Perplexity: 258.20\n","Epoch [34/200], Loss: 5.5422, Perplexity: 255.23\n","Epoch [35/200], Loss: 5.5308, Perplexity: 252.36\n","Epoch [36/200], Loss: 5.5197, Perplexity: 249.57\n","Epoch [37/200], Loss: 5.5088, Perplexity: 246.87\n","Epoch [38/200], Loss: 5.4982, Perplexity: 244.24\n","Epoch [39/200], Loss: 5.4877, Perplexity: 241.69\n","Epoch [40/200], Loss: 5.4774, Perplexity: 239.22\n","Epoch [41/200], Loss: 5.4672, Perplexity: 236.81\n","Epoch [42/200], Loss: 5.4573, Perplexity: 234.47\n","Epoch [43/200], Loss: 5.4476, Perplexity: 232.19\n","Epoch [44/200], Loss: 5.4380, Perplexity: 229.97\n","Epoch [45/200], Loss: 5.4285, Perplexity: 227.82\n","Epoch [46/200], Loss: 5.4193, Perplexity: 225.72\n","Epoch [47/200], Loss: 5.4102, Perplexity: 223.68\n","Epoch [48/200], Loss: 5.4013, Perplexity: 221.69\n","Epoch [49/200], Loss: 5.3925, Perplexity: 219.75\n","Epoch [50/200], Loss: 5.3838, Perplexity: 217.85\n","Epoch [51/200], Loss: 5.3752, Perplexity: 215.98\n","Epoch [52/200], Loss: 5.3668, Perplexity: 214.17\n","Epoch [53/200], Loss: 5.3585, Perplexity: 212.41\n","Epoch [54/200], Loss: 5.3504, Perplexity: 210.69\n","Epoch [55/200], Loss: 5.3424, Perplexity: 209.01\n","Epoch [56/200], Loss: 5.3345, Perplexity: 207.38\n","Epoch [57/200], Loss: 5.3268, Perplexity: 205.78\n","Epoch [58/200], Loss: 5.3192, Perplexity: 204.21\n","Epoch [59/200], Loss: 5.3116, Perplexity: 202.68\n","Epoch [60/200], Loss: 5.3042, Perplexity: 201.18\n","Epoch [61/200], Loss: 5.2969, Perplexity: 199.72\n","Epoch [62/200], Loss: 5.2897, Perplexity: 198.28\n","Epoch [63/200], Loss: 5.2826, Perplexity: 196.87\n","Epoch [64/200], Loss: 5.2755, Perplexity: 195.49\n","Epoch [65/200], Loss: 5.2686, Perplexity: 194.14\n","Epoch [66/200], Loss: 5.2617, Perplexity: 192.81\n","Epoch [67/200], Loss: 5.2549, Perplexity: 191.50\n","Epoch [68/200], Loss: 5.2482, Perplexity: 190.22\n","Epoch [69/200], Loss: 5.2416, Perplexity: 188.97\n","Epoch [70/200], Loss: 5.2350, Perplexity: 187.73\n","Epoch [71/200], Loss: 5.2285, Perplexity: 186.52\n","Epoch [72/200], Loss: 5.2221, Perplexity: 185.33\n","Epoch [73/200], Loss: 5.2158, Perplexity: 184.16\n","Epoch [74/200], Loss: 5.2095, Perplexity: 183.01\n","Epoch [75/200], Loss: 5.2033, Perplexity: 181.88\n","Epoch [76/200], Loss: 5.1972, Perplexity: 180.76\n","Epoch [77/200], Loss: 5.1911, Perplexity: 179.67\n","Epoch [78/200], Loss: 5.1851, Perplexity: 178.59\n","Epoch [79/200], Loss: 5.1791, Perplexity: 177.53\n","Epoch [80/200], Loss: 5.1732, Perplexity: 176.48\n","Epoch [81/200], Loss: 5.1674, Perplexity: 175.45\n","Epoch [82/200], Loss: 5.1616, Perplexity: 174.44\n","Epoch [83/200], Loss: 5.1558, Perplexity: 173.44\n","Epoch [84/200], Loss: 5.1502, Perplexity: 172.46\n","Epoch [85/200], Loss: 5.1445, Perplexity: 171.49\n","Epoch [86/200], Loss: 5.1389, Perplexity: 170.54\n","Epoch [87/200], Loss: 5.1334, Perplexity: 169.59\n","Epoch [88/200], Loss: 5.1279, Perplexity: 168.67\n","Epoch [89/200], Loss: 5.1225, Perplexity: 167.75\n","Epoch [90/200], Loss: 5.1171, Perplexity: 166.85\n","Epoch [91/200], Loss: 5.1117, Perplexity: 165.96\n","Epoch [92/200], Loss: 5.1064, Perplexity: 165.08\n","Epoch [93/200], Loss: 5.1012, Perplexity: 164.21\n","Epoch [94/200], Loss: 5.0959, Perplexity: 163.36\n","Epoch [95/200], Loss: 5.0908, Perplexity: 162.51\n","Epoch [96/200], Loss: 5.0856, Perplexity: 161.68\n","Epoch [97/200], Loss: 5.0805, Perplexity: 160.86\n","Epoch [98/200], Loss: 5.0754, Perplexity: 160.04\n","Epoch [99/200], Loss: 5.0704, Perplexity: 159.24\n","Epoch [100/200], Loss: 5.0654, Perplexity: 158.44\n","Epoch [101/200], Loss: 5.0604, Perplexity: 157.66\n","Epoch [102/200], Loss: 5.0555, Perplexity: 156.88\n","Epoch [103/200], Loss: 5.0506, Perplexity: 156.12\n","Epoch [104/200], Loss: 5.0457, Perplexity: 155.36\n","Epoch [105/200], Loss: 5.0409, Perplexity: 154.61\n","Epoch [106/200], Loss: 5.0361, Perplexity: 153.87\n","Epoch [107/200], Loss: 5.0313, Perplexity: 153.14\n","Epoch [108/200], Loss: 5.0266, Perplexity: 152.41\n","Epoch [109/200], Loss: 5.0219, Perplexity: 151.69\n","Epoch [110/200], Loss: 5.0172, Perplexity: 150.98\n","Epoch [111/200], Loss: 5.0125, Perplexity: 150.28\n","Epoch [112/200], Loss: 5.0079, Perplexity: 149.59\n","Epoch [113/200], Loss: 5.0033, Perplexity: 148.90\n","Epoch [114/200], Loss: 4.9987, Perplexity: 148.22\n","Epoch [115/200], Loss: 4.9942, Perplexity: 147.55\n","Epoch [116/200], Loss: 4.9897, Perplexity: 146.89\n","Epoch [117/200], Loss: 4.9852, Perplexity: 146.23\n","Epoch [118/200], Loss: 4.9807, Perplexity: 145.58\n","Epoch [119/200], Loss: 4.9763, Perplexity: 144.93\n","Epoch [120/200], Loss: 4.9719, Perplexity: 144.30\n","Epoch [121/200], Loss: 4.9675, Perplexity: 143.66\n","Epoch [122/200], Loss: 4.9631, Perplexity: 143.04\n","Epoch [123/200], Loss: 4.9588, Perplexity: 142.42\n","Epoch [124/200], Loss: 4.9545, Perplexity: 141.81\n","Epoch [125/200], Loss: 4.9502, Perplexity: 141.21\n","Epoch [126/200], Loss: 4.9460, Perplexity: 140.61\n","Epoch [127/200], Loss: 4.9417, Perplexity: 140.01\n","Epoch [128/200], Loss: 4.9375, Perplexity: 139.43\n","Epoch [129/200], Loss: 4.9334, Perplexity: 138.85\n","Epoch [130/200], Loss: 4.9292, Perplexity: 138.27\n","Epoch [131/200], Loss: 4.9251, Perplexity: 137.70\n","Epoch [132/200], Loss: 4.9210, Perplexity: 137.14\n","Epoch [133/200], Loss: 4.9169, Perplexity: 136.58\n","Epoch [134/200], Loss: 4.9129, Perplexity: 136.03\n","Epoch [135/200], Loss: 4.9088, Perplexity: 135.48\n","Epoch [136/200], Loss: 4.9048, Perplexity: 134.94\n","Epoch [137/200], Loss: 4.9008, Perplexity: 134.40\n","Epoch [138/200], Loss: 4.8969, Perplexity: 133.87\n","Epoch [139/200], Loss: 4.8929, Perplexity: 133.34\n","Epoch [140/200], Loss: 4.8890, Perplexity: 132.82\n","Epoch [141/200], Loss: 4.8851, Perplexity: 132.30\n","Epoch [142/200], Loss: 4.8812, Perplexity: 131.79\n","Epoch [143/200], Loss: 4.8773, Perplexity: 131.28\n","Epoch [144/200], Loss: 4.8735, Perplexity: 130.78\n","Epoch [145/200], Loss: 4.8697, Perplexity: 130.28\n","Epoch [146/200], Loss: 4.8659, Perplexity: 129.78\n","Epoch [147/200], Loss: 4.8621, Perplexity: 129.29\n","Epoch [148/200], Loss: 4.8583, Perplexity: 128.81\n","Epoch [149/200], Loss: 4.8546, Perplexity: 128.32\n","Epoch [150/200], Loss: 4.8508, Perplexity: 127.85\n","Epoch [151/200], Loss: 4.8471, Perplexity: 127.37\n","Epoch [152/200], Loss: 4.8434, Perplexity: 126.90\n","Epoch [153/200], Loss: 4.8398, Perplexity: 126.44\n","Epoch [154/200], Loss: 4.8361, Perplexity: 125.98\n","Epoch [155/200], Loss: 4.8325, Perplexity: 125.52\n","Epoch [156/200], Loss: 4.8289, Perplexity: 125.07\n","Epoch [157/200], Loss: 4.8253, Perplexity: 124.62\n","Epoch [158/200], Loss: 4.8217, Perplexity: 124.18\n","Epoch [159/200], Loss: 4.8182, Perplexity: 123.74\n","Epoch [160/200], Loss: 4.8147, Perplexity: 123.30\n","Epoch [161/200], Loss: 4.8111, Perplexity: 122.87\n","Epoch [162/200], Loss: 4.8076, Perplexity: 122.44\n","Epoch [163/200], Loss: 4.8042, Perplexity: 122.02\n","Epoch [164/200], Loss: 4.8007, Perplexity: 121.60\n","Epoch [165/200], Loss: 4.7973, Perplexity: 121.18\n","Epoch [166/200], Loss: 4.7939, Perplexity: 120.77\n","Epoch [167/200], Loss: 4.7905, Perplexity: 120.36\n","Epoch [168/200], Loss: 4.7871, Perplexity: 119.95\n","Epoch [169/200], Loss: 4.7837, Perplexity: 119.55\n","Epoch [170/200], Loss: 4.7803, Perplexity: 119.15\n","Epoch [171/200], Loss: 4.7770, Perplexity: 118.75\n","Epoch [172/200], Loss: 4.7737, Perplexity: 118.36\n","Epoch [173/200], Loss: 4.7704, Perplexity: 117.97\n","Epoch [174/200], Loss: 4.7671, Perplexity: 117.58\n","Epoch [175/200], Loss: 4.7638, Perplexity: 117.19\n","Epoch [176/200], Loss: 4.7606, Perplexity: 116.81\n","Epoch [177/200], Loss: 4.7573, Perplexity: 116.44\n","Epoch [178/200], Loss: 4.7541, Perplexity: 116.06\n","Epoch [179/200], Loss: 4.7509, Perplexity: 115.69\n","Epoch [180/200], Loss: 4.7477, Perplexity: 115.32\n","Epoch [181/200], Loss: 4.7445, Perplexity: 114.95\n","Epoch [182/200], Loss: 4.7414, Perplexity: 114.59\n","Epoch [183/200], Loss: 4.7382, Perplexity: 114.23\n","Epoch [184/200], Loss: 4.7351, Perplexity: 113.87\n","Epoch [185/200], Loss: 4.7320, Perplexity: 113.52\n","Epoch [186/200], Loss: 4.7288, Perplexity: 113.17\n","Epoch [187/200], Loss: 4.7258, Perplexity: 112.82\n","Epoch [188/200], Loss: 4.7227, Perplexity: 112.47\n","Epoch [189/200], Loss: 4.7196, Perplexity: 112.13\n","Epoch [190/200], Loss: 4.7166, Perplexity: 111.78\n","Epoch [191/200], Loss: 4.7135, Perplexity: 111.45\n","Epoch [192/200], Loss: 4.7105, Perplexity: 111.11\n","Epoch [193/200], Loss: 4.7075, Perplexity: 110.78\n","Epoch [194/200], Loss: 4.7045, Perplexity: 110.44\n","Epoch [195/200], Loss: 4.7015, Perplexity: 110.12\n","Epoch [196/200], Loss: 4.6986, Perplexity: 109.79\n","Epoch [197/200], Loss: 4.6956, Perplexity: 109.47\n","Epoch [198/200], Loss: 4.6927, Perplexity: 109.14\n","Epoch [199/200], Loss: 4.6897, Perplexity: 108.83\n","Epoch [200/200], Loss: 4.6868, Perplexity: 108.51\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KUPNfDYqVVOM","colab_type":"code","colab":{}},"source":["import pandas as pd\n","\n","frame = pd.DataFrame(zip(train_loss_lstm_list,train_perp_lstm_list))\n","frame.columns = ['train_loss','train_perp']\n","frame.to_csv('1_LSTM(AMSBound).csv')\n","\n","# plt.figure(figsize=(10,6))\n","# plt.title('SGD default lr, 200 epochs, reduce lr on plateau on 150 epoch')\n","# plt.plot(range(3),frame['train_loss'])\n","\n","\n","# plt.figure(figsize=(10,6))\n","# plt.title('SGD default lr, 200 epochs, reduce lr on plateau on 150 epoch')\n","# plt.plot(range(3),frame['train_perp'])"],"execution_count":0,"outputs":[]}]}