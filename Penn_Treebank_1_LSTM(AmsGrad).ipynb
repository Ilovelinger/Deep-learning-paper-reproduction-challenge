{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Penn_Treebank_1_LSTM(AmsGrad).ipynb","provenance":[{"file_id":"1PVTmJ94PgoIHVW1Q315a7Xof_wuf05Oq","timestamp":1590439281279},{"file_id":"1C7tl3uUJ2eh-A39b93b0JPx8qy8RuYdN","timestamp":1590433263405},{"file_id":"18_0qHuK5tvwAfyVVoruWVta1pxBzDQxC","timestamp":1590419745097}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"S6lyfG8jZt1b","colab_type":"text"},"source":["# Init"]},{"cell_type":"code","metadata":{"id":"usrX2f-qZvi5","colab_type":"code","outputId":"f2315d79-6cac-47c9-c6bc-88c176225140","executionInfo":{"status":"ok","timestamp":1590439377260,"user_tz":-60,"elapsed":8459,"user":{"displayName":"Eric Lee","photoUrl":"","userId":"15439544951905613244"}},"colab":{"base_uri":"https://localhost:8080/","height":168}},"source":["# Execute this code block to install dependencies when running on colab\n","try:\n","    import torch\n","except:\n","    from os.path import exists\n","    from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n","    platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n","    cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n","    accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n","\n","    !pip install -q http://download.pytorch.org/whl/{accelerator}/torch-1.0.0-{platform}-linux_x86_64.whl torchvision\n","\n","try: \n","    import torchbearer\n","except:\n","    !pip install torchbearer"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting torchbearer\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/e9/4049a47dd2e5b6346a2c5d215b0c67dce814afbab1cd54ce024533c4834e/torchbearer-0.5.3-py3-none-any.whl (138kB)\n","\r\u001b[K     |██▍                             | 10kB 17.6MB/s eta 0:00:01\r\u001b[K     |████▊                           | 20kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 30kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 40kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 51kB 6.1MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 61kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 71kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 81kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 92kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 102kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 112kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 122kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 133kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 143kB 8.5MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from torchbearer) (1.5.0+cu101)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchbearer) (1.18.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchbearer) (4.41.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->torchbearer) (0.16.0)\n","Installing collected packages: torchbearer\n","Successfully installed torchbearer-0.5.3\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RQV9LbVMZ5NG","colab_type":"text"},"source":["# Prepare Penn Treebank dataset"]},{"cell_type":"code","metadata":{"id":"KhEeh6x8Z9eE","colab_type":"code","colab":{}},"source":["# automatically reload external modules if they change\n","%load_ext autoreload\n","%autoreload 2\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torchtext import data\n","from torchtext import vocab\n","from torchtext import datasets\n","\n","import numpy as np\n","from matplotlib import pyplot as plt\n","\n","from tqdm import tqdm"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NWmejPqvd93t","colab_type":"code","outputId":"83d42942-760e-4ffd-877b-2f0f7e01f119","executionInfo":{"status":"ok","timestamp":1590439841643,"user_tz":-60,"elapsed":472830,"user":{"displayName":"Eric Lee","photoUrl":"","userId":"15439544951905613244"}},"colab":{"base_uri":"https://localhost:8080/","height":168}},"source":["tokenize = lambda x: x.split()\n","TEXT = data.Field(sequential = True, tokenize = tokenize, lower = True, batch_first = True)\n","train_dataset, val_dataset, test_dataset = datasets.PennTreebank.splits(TEXT)\n","TEXT.build_vocab(train_dataset, vectors=vocab.GloVe(name='6B', dim=300))\n","\n","vocab_size = len(TEXT.vocab)\n","word_embeddings = TEXT.vocab.vectors\n","print(vocab_size)\n","print(word_embeddings.size())\n","embeddings_length = 300\n","hidden_size = 256\n","batch_size = 32"],"execution_count":0,"outputs":[{"output_type":"stream","text":["downloading ptb.train.txt\n"],"name":"stdout"},{"output_type":"stream","text":["ptb.train.txt: 5.10MB [00:00, 54.2MB/s]                   \n"],"name":"stderr"},{"output_type":"stream","text":["downloading ptb.valid.txt\n"],"name":"stdout"},{"output_type":"stream","text":["ptb.valid.txt: 400kB [00:00, 24.9MB/s]                   \n"],"name":"stderr"},{"output_type":"stream","text":["downloading ptb.test.txt\n"],"name":"stdout"},{"output_type":"stream","text":["ptb.test.txt: 450kB [00:00, 25.0MB/s]                   \n",".vector_cache/glove.6B.zip: 862MB [06:30, 2.21MB/s]                           \n","100%|█████████▉| 399957/400000 [00:38<00:00, 10514.72it/s]"],"name":"stderr"},{"output_type":"stream","text":["10001\n","torch.Size([10001, 300])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5Hl-iXexfSWl","colab_type":"code","colab":{}},"source":["train_iter, val_iter, test_iter = data.BPTTIterator.splits((train_dataset, val_dataset, test_dataset), batch_size = 32, bptt_len=30, repeat=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wPLZIYqKhvTF","colab_type":"code","colab":{}},"source":["class LstmLangModel(nn.Module):\n","   def __init__(self, batch_size, hidden_size, vocab_size, embeddings_length, weights):\n","       super(LstmLangModel, self).__init__()\n","       self.batch_size = batch_size\n","       self.hidden_size = hidden_size\n","       self.vocab_size = vocab_size\n","       self.embed = nn.Embedding(vocab_size, embeddings_length)\n","       self.embed.weight.data.copy_(weights)\n","       self.lstm = nn.LSTM(embeddings_length, hidden_size, batch_first=True)\n","       self.fc = nn.Linear(hidden_size, vocab_size)\n","   def forward(self, x, h):\n","       x = self.embed(x)\n","       output_seq, (h, c) = self.lstm(x, h)\n","       out = output_seq.reshape(output_seq.size(0)*output_seq.size(1), output_seq.size(2))\n","       out = self.fc(out)\n","       return out, (h, c)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ySgb_BtfkbjZ","colab_type":"code","outputId":"1b30da37-d7e4-4ced-d8d2-143e4696da3b","executionInfo":{"status":"ok","timestamp":1590439841647,"user_tz":-60,"elapsed":472822,"user":{"displayName":"Eric Lee","photoUrl":"","userId":"15439544951905613244"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["vocab_size = len(TEXT.vocab)\n","word_embeddings = TEXT.vocab.vectors\n","print(vocab_size)\n","print(word_embeddings.size())\n","embeddings_length = 300\n","hidden_size = 256\n","batch_size = 32"],"execution_count":0,"outputs":[{"output_type":"stream","text":["10001\n","torch.Size([10001, 300])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"O7h9zfUYkFEc","colab_type":"code","outputId":"3410ebd0-1c79-4e14-be62-0741cedeea0c","executionInfo":{"status":"ok","timestamp":1590439841648,"user_tz":-60,"elapsed":472814,"user":{"displayName":"Eric Lee","photoUrl":"","userId":"15439544951905613244"}},"colab":{"base_uri":"https://localhost:8080/","height":101}},"source":["model = LstmLangModel(batch_size, hidden_size, vocab_size, embeddings_length, word_embeddings)\n","model.eval()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LstmLangModel(\n","  (embed): Embedding(10001, 300)\n","  (lstm): LSTM(300, 256, batch_first=True)\n","  (fc): Linear(in_features=256, out_features=10001, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"k6CWQqenmQ56","colab_type":"text"},"source":["# Torchbear Section"]},{"cell_type":"code","metadata":{"id":"TtOu1hMbjzbZ","colab_type":"code","outputId":"814c2333-e44e-44c6-c0d8-f1ed49084184","executionInfo":{"status":"ok","timestamp":1590441871770,"user_tz":-60,"elapsed":2502928,"user":{"displayName":"Eric Lee","photoUrl":"","userId":"15439544951905613244"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","net_lstm = LstmLangModel(batch_size, hidden_size, vocab_size, embeddings_length, word_embeddings)\n","net_lstm = net_lstm.to(device)\n","criterion = nn.CrossEntropyLoss()\n","optim = torch.optim.Adam(filter(lambda p: p.requires_grad, net_lstm.parameters()),lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=True)\n","num_epochs = 200\n","epoch_list = []\n","train_loss_lstm_list = []\n","train_perp_lstm_list = []\n","\n","def detach(states):\n","   return [state.detach() for state in states]\n","\n","\n","\n","for epoch in range(num_epochs):\n","   train_loss = 0\n","   states = (torch.zeros(1, batch_size, hidden_size).to(device),\n","             torch.zeros(1, batch_size, hidden_size).to(device))\n","   net_lstm.train()\n","\n","   for i, batch in enumerate(train_iter):\n","       text = batch.text.to(device)\n","       labels = batch.target.to(device)\n","       text = text.permute(1, 0)\n","       labels = labels.permute(1, 0)\n","\n","       optim.zero_grad()\n","       states = detach(states)\n","       outputs, states = net_lstm(text, states)\n","       loss = criterion(outputs, labels.reshape(-1))\n","       train_loss += loss.item()\n","       loss.backward()\n","       optim.step()\n","   avg_train_loss = train_loss / len(train_iter)\n","   perplexity = np.exp(avg_train_loss)\n","   print('Epoch [{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'.format(epoch + 1, num_epochs, avg_train_loss, perplexity))\n","   train_loss_lstm_list.append(avg_train_loss)\n","   train_perp_lstm_list.append(perplexity)\n","\n","   if epoch % 100 == 0:\n","       torch.save(net_lstm.state_dict(), r\"./LSTM_\" + str(epoch) + r\".pth\")\n","   \n","   if epoch == num_epochs - 1:\n","       torch.save(net_lstm.state_dict(), r\"./LSTM_\" + str(epoch) + r\".pth\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\r100%|█████████▉| 399957/400000 [00:50<00:00, 10514.72it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch [1/200], Loss: 5.7943, Perplexity: 328.42\n","Epoch [2/200], Loss: 5.0785, Perplexity: 160.53\n","Epoch [3/200], Loss: 4.7854, Perplexity: 119.74\n","Epoch [4/200], Loss: 4.5722, Perplexity: 96.76\n","Epoch [5/200], Loss: 4.4042, Perplexity: 81.79\n","Epoch [6/200], Loss: 4.2633, Perplexity: 71.04\n","Epoch [7/200], Loss: 4.1399, Perplexity: 62.79\n","Epoch [8/200], Loss: 4.0291, Perplexity: 56.21\n","Epoch [9/200], Loss: 3.9284, Perplexity: 50.83\n","Epoch [10/200], Loss: 3.8350, Perplexity: 46.29\n","Epoch [11/200], Loss: 3.7477, Perplexity: 42.42\n","Epoch [12/200], Loss: 3.6647, Perplexity: 39.04\n","Epoch [13/200], Loss: 3.5860, Perplexity: 36.09\n","Epoch [14/200], Loss: 3.5120, Perplexity: 33.51\n","Epoch [15/200], Loss: 3.4421, Perplexity: 31.25\n","Epoch [16/200], Loss: 3.3762, Perplexity: 29.26\n","Epoch [17/200], Loss: 3.3126, Perplexity: 27.46\n","Epoch [18/200], Loss: 3.2520, Perplexity: 25.84\n","Epoch [19/200], Loss: 3.1947, Perplexity: 24.40\n","Epoch [20/200], Loss: 3.1403, Perplexity: 23.11\n","Epoch [21/200], Loss: 3.0868, Perplexity: 21.91\n","Epoch [22/200], Loss: 3.0347, Perplexity: 20.80\n","Epoch [23/200], Loss: 2.9838, Perplexity: 19.76\n","Epoch [24/200], Loss: 2.9350, Perplexity: 18.82\n","Epoch [25/200], Loss: 2.8892, Perplexity: 17.98\n","Epoch [26/200], Loss: 2.8445, Perplexity: 17.19\n","Epoch [27/200], Loss: 2.8010, Perplexity: 16.46\n","Epoch [28/200], Loss: 2.7594, Perplexity: 15.79\n","Epoch [29/200], Loss: 2.7204, Perplexity: 15.19\n","Epoch [30/200], Loss: 2.6836, Perplexity: 14.64\n","Epoch [31/200], Loss: 2.6485, Perplexity: 14.13\n","Epoch [32/200], Loss: 2.6145, Perplexity: 13.66\n","Epoch [33/200], Loss: 2.5814, Perplexity: 13.22\n","Epoch [34/200], Loss: 2.5504, Perplexity: 12.81\n","Epoch [35/200], Loss: 2.5218, Perplexity: 12.45\n","Epoch [36/200], Loss: 2.4927, Perplexity: 12.09\n","Epoch [37/200], Loss: 2.4640, Perplexity: 11.75\n","Epoch [38/200], Loss: 2.4359, Perplexity: 11.43\n","Epoch [39/200], Loss: 2.4081, Perplexity: 11.11\n","Epoch [40/200], Loss: 2.3809, Perplexity: 10.81\n","Epoch [41/200], Loss: 2.3578, Perplexity: 10.57\n","Epoch [42/200], Loss: 2.3344, Perplexity: 10.32\n","Epoch [43/200], Loss: 2.3124, Perplexity: 10.10\n","Epoch [44/200], Loss: 2.2909, Perplexity:  9.88\n","Epoch [45/200], Loss: 2.2703, Perplexity:  9.68\n","Epoch [46/200], Loss: 2.2500, Perplexity:  9.49\n","Epoch [47/200], Loss: 2.2315, Perplexity:  9.31\n","Epoch [48/200], Loss: 2.2143, Perplexity:  9.16\n","Epoch [49/200], Loss: 2.1938, Perplexity:  8.97\n","Epoch [50/200], Loss: 2.1761, Perplexity:  8.81\n","Epoch [51/200], Loss: 2.1577, Perplexity:  8.65\n","Epoch [52/200], Loss: 2.1404, Perplexity:  8.50\n","Epoch [53/200], Loss: 2.1251, Perplexity:  8.37\n","Epoch [54/200], Loss: 2.1088, Perplexity:  8.24\n","Epoch [55/200], Loss: 2.0926, Perplexity:  8.11\n","Epoch [56/200], Loss: 2.0790, Perplexity:  8.00\n","Epoch [57/200], Loss: 2.0644, Perplexity:  7.88\n","Epoch [58/200], Loss: 2.0514, Perplexity:  7.78\n","Epoch [59/200], Loss: 2.0378, Perplexity:  7.67\n","Epoch [60/200], Loss: 2.0254, Perplexity:  7.58\n","Epoch [61/200], Loss: 2.0129, Perplexity:  7.49\n","Epoch [62/200], Loss: 2.0004, Perplexity:  7.39\n","Epoch [63/200], Loss: 1.9890, Perplexity:  7.31\n","Epoch [64/200], Loss: 1.9797, Perplexity:  7.24\n","Epoch [65/200], Loss: 1.9676, Perplexity:  7.15\n","Epoch [66/200], Loss: 1.9569, Perplexity:  7.08\n","Epoch [67/200], Loss: 1.9471, Perplexity:  7.01\n","Epoch [68/200], Loss: 1.9381, Perplexity:  6.95\n","Epoch [69/200], Loss: 1.9280, Perplexity:  6.88\n","Epoch [70/200], Loss: 1.9194, Perplexity:  6.82\n","Epoch [71/200], Loss: 1.9081, Perplexity:  6.74\n","Epoch [72/200], Loss: 1.8985, Perplexity:  6.68\n","Epoch [73/200], Loss: 1.8905, Perplexity:  6.62\n","Epoch [74/200], Loss: 1.8822, Perplexity:  6.57\n","Epoch [75/200], Loss: 1.8730, Perplexity:  6.51\n","Epoch [76/200], Loss: 1.8651, Perplexity:  6.46\n","Epoch [77/200], Loss: 1.8560, Perplexity:  6.40\n","Epoch [78/200], Loss: 1.8498, Perplexity:  6.36\n","Epoch [79/200], Loss: 1.8404, Perplexity:  6.30\n","Epoch [80/200], Loss: 1.8333, Perplexity:  6.25\n","Epoch [81/200], Loss: 1.8245, Perplexity:  6.20\n","Epoch [82/200], Loss: 1.8167, Perplexity:  6.15\n","Epoch [83/200], Loss: 1.8098, Perplexity:  6.11\n","Epoch [84/200], Loss: 1.8032, Perplexity:  6.07\n","Epoch [85/200], Loss: 1.7972, Perplexity:  6.03\n","Epoch [86/200], Loss: 1.7919, Perplexity:  6.00\n","Epoch [87/200], Loss: 1.7850, Perplexity:  5.96\n","Epoch [88/200], Loss: 1.7793, Perplexity:  5.93\n","Epoch [89/200], Loss: 1.7710, Perplexity:  5.88\n","Epoch [90/200], Loss: 1.7641, Perplexity:  5.84\n","Epoch [91/200], Loss: 1.7562, Perplexity:  5.79\n","Epoch [92/200], Loss: 1.7527, Perplexity:  5.77\n","Epoch [93/200], Loss: 1.7443, Perplexity:  5.72\n","Epoch [94/200], Loss: 1.7391, Perplexity:  5.69\n","Epoch [95/200], Loss: 1.7329, Perplexity:  5.66\n","Epoch [96/200], Loss: 1.7287, Perplexity:  5.63\n","Epoch [97/200], Loss: 1.7250, Perplexity:  5.61\n","Epoch [98/200], Loss: 1.7199, Perplexity:  5.58\n","Epoch [99/200], Loss: 1.7151, Perplexity:  5.56\n","Epoch [100/200], Loss: 1.7105, Perplexity:  5.53\n","Epoch [101/200], Loss: 1.7060, Perplexity:  5.51\n","Epoch [102/200], Loss: 1.6989, Perplexity:  5.47\n","Epoch [103/200], Loss: 1.6946, Perplexity:  5.44\n","Epoch [104/200], Loss: 1.6876, Perplexity:  5.41\n","Epoch [105/200], Loss: 1.6801, Perplexity:  5.37\n","Epoch [106/200], Loss: 1.6787, Perplexity:  5.36\n","Epoch [107/200], Loss: 1.6731, Perplexity:  5.33\n","Epoch [108/200], Loss: 1.6706, Perplexity:  5.32\n","Epoch [109/200], Loss: 1.6666, Perplexity:  5.29\n","Epoch [110/200], Loss: 1.6603, Perplexity:  5.26\n","Epoch [111/200], Loss: 1.6579, Perplexity:  5.25\n","Epoch [112/200], Loss: 1.6533, Perplexity:  5.22\n","Epoch [113/200], Loss: 1.6502, Perplexity:  5.21\n","Epoch [114/200], Loss: 1.6452, Perplexity:  5.18\n","Epoch [115/200], Loss: 1.6414, Perplexity:  5.16\n","Epoch [116/200], Loss: 1.6356, Perplexity:  5.13\n","Epoch [117/200], Loss: 1.6312, Perplexity:  5.11\n","Epoch [118/200], Loss: 1.6278, Perplexity:  5.09\n","Epoch [119/200], Loss: 1.6250, Perplexity:  5.08\n","Epoch [120/200], Loss: 1.6195, Perplexity:  5.05\n","Epoch [121/200], Loss: 1.6165, Perplexity:  5.04\n","Epoch [122/200], Loss: 1.6129, Perplexity:  5.02\n","Epoch [123/200], Loss: 1.6076, Perplexity:  4.99\n","Epoch [124/200], Loss: 1.6040, Perplexity:  4.97\n","Epoch [125/200], Loss: 1.6013, Perplexity:  4.96\n","Epoch [126/200], Loss: 1.5995, Perplexity:  4.95\n","Epoch [127/200], Loss: 1.5902, Perplexity:  4.90\n","Epoch [128/200], Loss: 1.5869, Perplexity:  4.89\n","Epoch [129/200], Loss: 1.5873, Perplexity:  4.89\n","Epoch [130/200], Loss: 1.5842, Perplexity:  4.88\n","Epoch [131/200], Loss: 1.5790, Perplexity:  4.85\n","Epoch [132/200], Loss: 1.5757, Perplexity:  4.83\n","Epoch [133/200], Loss: 1.5724, Perplexity:  4.82\n","Epoch [134/200], Loss: 1.5693, Perplexity:  4.80\n","Epoch [135/200], Loss: 1.5662, Perplexity:  4.79\n","Epoch [136/200], Loss: 1.5628, Perplexity:  4.77\n","Epoch [137/200], Loss: 1.5658, Perplexity:  4.79\n","Epoch [138/200], Loss: 1.5593, Perplexity:  4.76\n","Epoch [139/200], Loss: 1.5575, Perplexity:  4.75\n","Epoch [140/200], Loss: 1.5529, Perplexity:  4.73\n","Epoch [141/200], Loss: 1.5499, Perplexity:  4.71\n","Epoch [142/200], Loss: 1.5479, Perplexity:  4.70\n","Epoch [143/200], Loss: 1.5442, Perplexity:  4.68\n","Epoch [144/200], Loss: 1.5406, Perplexity:  4.67\n","Epoch [145/200], Loss: 1.5358, Perplexity:  4.65\n","Epoch [146/200], Loss: 1.5312, Perplexity:  4.62\n","Epoch [147/200], Loss: 1.5310, Perplexity:  4.62\n","Epoch [148/200], Loss: 1.5299, Perplexity:  4.62\n","Epoch [149/200], Loss: 1.5283, Perplexity:  4.61\n","Epoch [150/200], Loss: 1.5254, Perplexity:  4.60\n","Epoch [151/200], Loss: 1.5214, Perplexity:  4.58\n","Epoch [152/200], Loss: 1.5198, Perplexity:  4.57\n","Epoch [153/200], Loss: 1.5149, Perplexity:  4.55\n","Epoch [154/200], Loss: 1.5150, Perplexity:  4.55\n","Epoch [155/200], Loss: 1.5129, Perplexity:  4.54\n","Epoch [156/200], Loss: 1.5092, Perplexity:  4.52\n","Epoch [157/200], Loss: 1.5035, Perplexity:  4.50\n","Epoch [158/200], Loss: 1.5045, Perplexity:  4.50\n","Epoch [159/200], Loss: 1.5026, Perplexity:  4.49\n","Epoch [160/200], Loss: 1.4998, Perplexity:  4.48\n","Epoch [161/200], Loss: 1.4980, Perplexity:  4.47\n","Epoch [162/200], Loss: 1.4957, Perplexity:  4.46\n","Epoch [163/200], Loss: 1.4940, Perplexity:  4.45\n","Epoch [164/200], Loss: 1.4888, Perplexity:  4.43\n","Epoch [165/200], Loss: 1.4883, Perplexity:  4.43\n","Epoch [166/200], Loss: 1.4848, Perplexity:  4.41\n","Epoch [167/200], Loss: 1.4839, Perplexity:  4.41\n","Epoch [168/200], Loss: 1.4783, Perplexity:  4.39\n","Epoch [169/200], Loss: 1.4748, Perplexity:  4.37\n","Epoch [170/200], Loss: 1.4767, Perplexity:  4.38\n","Epoch [171/200], Loss: 1.4749, Perplexity:  4.37\n","Epoch [172/200], Loss: 1.4706, Perplexity:  4.35\n","Epoch [173/200], Loss: 1.4693, Perplexity:  4.35\n","Epoch [174/200], Loss: 1.4639, Perplexity:  4.32\n","Epoch [175/200], Loss: 1.4601, Perplexity:  4.31\n","Epoch [176/200], Loss: 1.4555, Perplexity:  4.29\n","Epoch [177/200], Loss: 1.4581, Perplexity:  4.30\n","Epoch [178/200], Loss: 1.4567, Perplexity:  4.29\n","Epoch [179/200], Loss: 1.4552, Perplexity:  4.29\n","Epoch [180/200], Loss: 1.4501, Perplexity:  4.26\n","Epoch [181/200], Loss: 1.4525, Perplexity:  4.27\n","Epoch [182/200], Loss: 1.4510, Perplexity:  4.27\n","Epoch [183/200], Loss: 1.4501, Perplexity:  4.26\n","Epoch [184/200], Loss: 1.4486, Perplexity:  4.26\n","Epoch [185/200], Loss: 1.4420, Perplexity:  4.23\n","Epoch [186/200], Loss: 1.4395, Perplexity:  4.22\n","Epoch [187/200], Loss: 1.4392, Perplexity:  4.22\n","Epoch [188/200], Loss: 1.4373, Perplexity:  4.21\n","Epoch [189/200], Loss: 1.4366, Perplexity:  4.21\n","Epoch [190/200], Loss: 1.4284, Perplexity:  4.17\n","Epoch [191/200], Loss: 1.4235, Perplexity:  4.15\n","Epoch [192/200], Loss: 1.4278, Perplexity:  4.17\n","Epoch [193/200], Loss: 1.4297, Perplexity:  4.18\n","Epoch [194/200], Loss: 1.4278, Perplexity:  4.17\n","Epoch [195/200], Loss: 1.4245, Perplexity:  4.16\n","Epoch [196/200], Loss: 1.4211, Perplexity:  4.14\n","Epoch [197/200], Loss: 1.4180, Perplexity:  4.13\n","Epoch [198/200], Loss: 1.4167, Perplexity:  4.12\n","Epoch [199/200], Loss: 1.4156, Perplexity:  4.12\n","Epoch [200/200], Loss: 1.4141, Perplexity:  4.11\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KUPNfDYqVVOM","colab_type":"code","colab":{}},"source":["import pandas as pd\n","\n","frame = pd.DataFrame(zip(train_loss_lstm_list,train_perp_lstm_list))\n","frame.columns = ['train_loss','train_perp']\n","frame.to_csv('1_LSTM(AmsGrad).csv')\n","\n","# plt.figure(figsize=(10,6))\n","# plt.title('SGD default lr, 200 epochs, reduce lr on plateau on 150 epoch')\n","# plt.plot(range(3),frame['train_loss'])\n","\n","\n","# plt.figure(figsize=(10,6))\n","# plt.title('SGD default lr, 200 epochs, reduce lr on plateau on 150 epoch')\n","# plt.plot(range(3),frame['train_perp'])"],"execution_count":0,"outputs":[]}]}